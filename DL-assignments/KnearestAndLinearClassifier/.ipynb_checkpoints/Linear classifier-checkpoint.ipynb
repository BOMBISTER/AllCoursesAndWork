{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13872\\3146107952.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(x*x), 2*x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.006760443547122)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397013\n",
      "Epoch 1, loss: 2.330839\n",
      "Epoch 2, loss: 2.310221\n",
      "Epoch 3, loss: 2.304235\n",
      "Epoch 4, loss: 2.302970\n",
      "Epoch 5, loss: 2.301240\n",
      "Epoch 6, loss: 2.303439\n",
      "Epoch 7, loss: 2.301806\n",
      "Epoch 8, loss: 2.301981\n",
      "Epoch 9, loss: 2.302028\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x147ad2d5430>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.123\n",
      "Epoch 0, loss: 2.302074\n",
      "Epoch 1, loss: 2.301790\n",
      "Epoch 2, loss: 2.302027\n",
      "Epoch 3, loss: 2.301962\n",
      "Epoch 4, loss: 2.302027\n",
      "Epoch 5, loss: 2.301657\n",
      "Epoch 6, loss: 2.301891\n",
      "Epoch 7, loss: 2.301901\n",
      "Epoch 8, loss: 2.301054\n",
      "Epoch 9, loss: 2.301645\n",
      "Epoch 10, loss: 2.301260\n",
      "Epoch 11, loss: 2.301721\n",
      "Epoch 12, loss: 2.301979\n",
      "Epoch 13, loss: 2.302828\n",
      "Epoch 14, loss: 2.301723\n",
      "Epoch 15, loss: 2.301501\n",
      "Epoch 16, loss: 2.302176\n",
      "Epoch 17, loss: 2.302477\n",
      "Epoch 18, loss: 2.301875\n",
      "Epoch 19, loss: 2.302851\n",
      "Epoch 20, loss: 2.301717\n",
      "Epoch 21, loss: 2.301771\n",
      "Epoch 22, loss: 2.302264\n",
      "Epoch 23, loss: 2.301561\n",
      "Epoch 24, loss: 2.301596\n",
      "Epoch 25, loss: 2.301920\n",
      "Epoch 26, loss: 2.302158\n",
      "Epoch 27, loss: 2.302007\n",
      "Epoch 28, loss: 2.302555\n",
      "Epoch 29, loss: 2.302493\n",
      "Epoch 30, loss: 2.302212\n",
      "Epoch 31, loss: 2.301803\n",
      "Epoch 32, loss: 2.302261\n",
      "Epoch 33, loss: 2.302014\n",
      "Epoch 34, loss: 2.301479\n",
      "Epoch 35, loss: 2.301749\n",
      "Epoch 36, loss: 2.302101\n",
      "Epoch 37, loss: 2.302706\n",
      "Epoch 38, loss: 2.301938\n",
      "Epoch 39, loss: 2.301584\n",
      "Epoch 40, loss: 2.302705\n",
      "Epoch 41, loss: 2.302255\n",
      "Epoch 42, loss: 2.302769\n",
      "Epoch 43, loss: 2.302105\n",
      "Epoch 44, loss: 2.303017\n",
      "Epoch 45, loss: 2.301576\n",
      "Epoch 46, loss: 2.301927\n",
      "Epoch 47, loss: 2.301987\n",
      "Epoch 48, loss: 2.301193\n",
      "Epoch 49, loss: 2.302175\n",
      "Epoch 50, loss: 2.302222\n",
      "Epoch 51, loss: 2.302859\n",
      "Epoch 52, loss: 2.301147\n",
      "Epoch 53, loss: 2.302409\n",
      "Epoch 54, loss: 2.302440\n",
      "Epoch 55, loss: 2.301894\n",
      "Epoch 56, loss: 2.301428\n",
      "Epoch 57, loss: 2.302344\n",
      "Epoch 58, loss: 2.301657\n",
      "Epoch 59, loss: 2.301483\n",
      "Epoch 60, loss: 2.301973\n",
      "Epoch 61, loss: 2.301651\n",
      "Epoch 62, loss: 2.301336\n",
      "Epoch 63, loss: 2.302211\n",
      "Epoch 64, loss: 2.301579\n",
      "Epoch 65, loss: 2.301768\n",
      "Epoch 66, loss: 2.302362\n",
      "Epoch 67, loss: 2.302270\n",
      "Epoch 68, loss: 2.302178\n",
      "Epoch 69, loss: 2.302523\n",
      "Epoch 70, loss: 2.301919\n",
      "Epoch 71, loss: 2.301701\n",
      "Epoch 72, loss: 2.302455\n",
      "Epoch 73, loss: 2.302778\n",
      "Epoch 74, loss: 2.302288\n",
      "Epoch 75, loss: 2.301505\n",
      "Epoch 76, loss: 2.302568\n",
      "Epoch 77, loss: 2.301574\n",
      "Epoch 78, loss: 2.301295\n",
      "Epoch 79, loss: 2.301726\n",
      "Epoch 80, loss: 2.302110\n",
      "Epoch 81, loss: 2.301866\n",
      "Epoch 82, loss: 2.301387\n",
      "Epoch 83, loss: 2.302556\n",
      "Epoch 84, loss: 2.302886\n",
      "Epoch 85, loss: 2.302011\n",
      "Epoch 86, loss: 2.301322\n",
      "Epoch 87, loss: 2.301509\n",
      "Epoch 88, loss: 2.302708\n",
      "Epoch 89, loss: 2.301951\n",
      "Epoch 90, loss: 2.301569\n",
      "Epoch 91, loss: 2.301078\n",
      "Epoch 92, loss: 2.302599\n",
      "Epoch 93, loss: 2.301967\n",
      "Epoch 94, loss: 2.301890\n",
      "Epoch 95, loss: 2.300662\n",
      "Epoch 96, loss: 2.301721\n",
      "Epoch 97, loss: 2.301311\n",
      "Epoch 98, loss: 2.300970\n",
      "Epoch 99, loss: 2.303150\n",
      "Accuracy after training for 100 epochs:  0.119\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.302108\n",
      "Epoch 1, loss: 2.300978\n",
      "Epoch 2, loss: 2.298565\n",
      "Epoch 3, loss: 2.296524\n",
      "Epoch 4, loss: 2.295413\n",
      "Epoch 5, loss: 2.294836\n",
      "Epoch 6, loss: 2.295870\n",
      "Epoch 7, loss: 2.297827\n",
      "Epoch 8, loss: 2.291598\n",
      "Epoch 9, loss: 2.296120\n",
      "Epoch 10, loss: 2.296082\n",
      "Epoch 11, loss: 2.293065\n",
      "Epoch 12, loss: 2.289324\n",
      "Epoch 13, loss: 2.292150\n",
      "Epoch 14, loss: 2.288675\n",
      "Epoch 15, loss: 2.291305\n",
      "Epoch 16, loss: 2.285007\n",
      "Epoch 17, loss: 2.287926\n",
      "Epoch 18, loss: 2.284230\n",
      "Epoch 19, loss: 2.284748\n",
      "Epoch 20, loss: 2.286946\n",
      "Epoch 21, loss: 2.283062\n",
      "Epoch 22, loss: 2.283947\n",
      "Epoch 23, loss: 2.279626\n",
      "Epoch 24, loss: 2.284460\n",
      "Epoch 25, loss: 2.285659\n",
      "Epoch 26, loss: 2.279039\n",
      "Epoch 27, loss: 2.279210\n",
      "Epoch 28, loss: 2.274641\n",
      "Epoch 29, loss: 2.283341\n",
      "Epoch 30, loss: 2.278239\n",
      "Epoch 31, loss: 2.278483\n",
      "Epoch 32, loss: 2.280101\n",
      "Epoch 33, loss: 2.276766\n",
      "Epoch 34, loss: 2.271577\n",
      "Epoch 35, loss: 2.277441\n",
      "Epoch 36, loss: 2.271227\n",
      "Epoch 37, loss: 2.277907\n",
      "Epoch 38, loss: 2.274241\n",
      "Epoch 39, loss: 2.271664\n",
      "Epoch 40, loss: 2.278085\n",
      "Epoch 41, loss: 2.272042\n",
      "Epoch 42, loss: 2.266297\n",
      "Epoch 43, loss: 2.265898\n",
      "Epoch 44, loss: 2.279775\n",
      "Epoch 45, loss: 2.272422\n",
      "Epoch 46, loss: 2.269812\n",
      "Epoch 47, loss: 2.269500\n",
      "Epoch 48, loss: 2.269948\n",
      "Epoch 49, loss: 2.269037\n",
      "Epoch 50, loss: 2.271655\n",
      "Epoch 51, loss: 2.263631\n",
      "Epoch 52, loss: 2.262797\n",
      "Epoch 53, loss: 2.258887\n",
      "Epoch 54, loss: 2.262597\n",
      "Epoch 55, loss: 2.247508\n",
      "Epoch 56, loss: 2.260037\n",
      "Epoch 57, loss: 2.253648\n",
      "Epoch 58, loss: 2.242671\n",
      "Epoch 59, loss: 2.262472\n",
      "Epoch 60, loss: 2.255230\n",
      "Epoch 61, loss: 2.270532\n",
      "Epoch 62, loss: 2.260979\n",
      "Epoch 63, loss: 2.258270\n",
      "Epoch 64, loss: 2.263647\n",
      "Epoch 65, loss: 2.263976\n",
      "Epoch 66, loss: 2.248059\n",
      "Epoch 67, loss: 2.260906\n",
      "Epoch 68, loss: 2.248155\n",
      "Epoch 69, loss: 2.255783\n",
      "Epoch 70, loss: 2.251087\n",
      "Epoch 71, loss: 2.257794\n",
      "Epoch 72, loss: 2.248725\n",
      "Epoch 73, loss: 2.262954\n",
      "Epoch 74, loss: 2.260854\n",
      "Epoch 75, loss: 2.257945\n",
      "Epoch 76, loss: 2.232517\n",
      "Epoch 77, loss: 2.266574\n",
      "Epoch 78, loss: 2.251340\n",
      "Epoch 79, loss: 2.247746\n",
      "Epoch 80, loss: 2.234660\n",
      "Epoch 81, loss: 2.254773\n",
      "Epoch 82, loss: 2.244422\n",
      "Epoch 83, loss: 2.262210\n",
      "Epoch 84, loss: 2.239807\n",
      "Epoch 85, loss: 2.248492\n",
      "Epoch 86, loss: 2.232402\n",
      "Epoch 87, loss: 2.242981\n",
      "Epoch 88, loss: 2.248048\n",
      "Epoch 89, loss: 2.213984\n",
      "Epoch 90, loss: 2.256448\n",
      "Epoch 91, loss: 2.240817\n",
      "Epoch 92, loss: 2.248809\n",
      "Epoch 93, loss: 2.225979\n",
      "Epoch 94, loss: 2.251240\n",
      "Epoch 95, loss: 2.232781\n",
      "Epoch 96, loss: 2.232244\n",
      "Epoch 97, loss: 2.253094\n",
      "Epoch 98, loss: 2.231626\n",
      "Epoch 99, loss: 2.228396\n",
      "Epoch 100, loss: 2.235436\n",
      "Epoch 101, loss: 2.243817\n",
      "Epoch 102, loss: 2.230024\n",
      "Epoch 103, loss: 2.229551\n",
      "Epoch 104, loss: 2.248007\n",
      "Epoch 105, loss: 2.222273\n",
      "Epoch 106, loss: 2.236848\n",
      "Epoch 107, loss: 2.239122\n",
      "Epoch 108, loss: 2.244184\n",
      "Epoch 109, loss: 2.254706\n",
      "Epoch 110, loss: 2.209704\n",
      "Epoch 111, loss: 2.231487\n",
      "Epoch 112, loss: 2.242841\n",
      "Epoch 113, loss: 2.256589\n",
      "Epoch 114, loss: 2.220862\n",
      "Epoch 115, loss: 2.237038\n",
      "Epoch 116, loss: 2.228485\n",
      "Epoch 117, loss: 2.228736\n",
      "Epoch 118, loss: 2.223818\n",
      "Epoch 119, loss: 2.220854\n",
      "Epoch 120, loss: 2.211154\n",
      "Epoch 121, loss: 2.226310\n",
      "Epoch 122, loss: 2.212449\n",
      "Epoch 123, loss: 2.209177\n",
      "Epoch 124, loss: 2.223051\n",
      "Epoch 125, loss: 2.223305\n",
      "Epoch 126, loss: 2.233390\n",
      "Epoch 127, loss: 2.225713\n",
      "Epoch 128, loss: 2.224406\n",
      "Epoch 129, loss: 2.206708\n",
      "Epoch 130, loss: 2.207375\n",
      "Epoch 131, loss: 2.222150\n",
      "Epoch 132, loss: 2.224165\n",
      "Epoch 133, loss: 2.220655\n",
      "Epoch 134, loss: 2.209922\n",
      "Epoch 135, loss: 2.224819\n",
      "Epoch 136, loss: 2.240099\n",
      "Epoch 137, loss: 2.215895\n",
      "Epoch 138, loss: 2.226781\n",
      "Epoch 139, loss: 2.225142\n",
      "Epoch 140, loss: 2.226731\n",
      "Epoch 141, loss: 2.223831\n",
      "Epoch 142, loss: 2.203263\n",
      "Epoch 143, loss: 2.233033\n",
      "Epoch 144, loss: 2.232306\n",
      "Epoch 145, loss: 2.226086\n",
      "Epoch 146, loss: 2.226323\n",
      "Epoch 147, loss: 2.222961\n",
      "Epoch 148, loss: 2.226807\n",
      "Epoch 149, loss: 2.232020\n",
      "Epoch 150, loss: 2.230779\n",
      "Epoch 151, loss: 2.221010\n",
      "Epoch 152, loss: 2.208751\n",
      "Epoch 153, loss: 2.206997\n",
      "Epoch 154, loss: 2.203202\n",
      "Epoch 155, loss: 2.205149\n",
      "Epoch 156, loss: 2.201224\n",
      "Epoch 157, loss: 2.237309\n",
      "Epoch 158, loss: 2.206707\n",
      "Epoch 159, loss: 2.202223\n",
      "Epoch 160, loss: 2.205621\n",
      "Epoch 161, loss: 2.214372\n",
      "Epoch 162, loss: 2.203159\n",
      "Epoch 163, loss: 2.189482\n",
      "Epoch 164, loss: 2.207618\n",
      "Epoch 165, loss: 2.201150\n",
      "Epoch 166, loss: 2.230997\n",
      "Epoch 167, loss: 2.202150\n",
      "Epoch 168, loss: 2.219967\n",
      "Epoch 169, loss: 2.203534\n",
      "Epoch 170, loss: 2.226569\n",
      "Epoch 171, loss: 2.208678\n",
      "Epoch 172, loss: 2.202350\n",
      "Epoch 173, loss: 2.221175\n",
      "Epoch 174, loss: 2.226059\n",
      "Epoch 175, loss: 2.223771\n",
      "Epoch 176, loss: 2.201716\n",
      "Epoch 177, loss: 2.225388\n",
      "Epoch 178, loss: 2.206198\n",
      "Epoch 179, loss: 2.211109\n",
      "Epoch 180, loss: 2.226243\n",
      "Epoch 181, loss: 2.195660\n",
      "Epoch 182, loss: 2.194490\n",
      "Epoch 183, loss: 2.201740\n",
      "Epoch 184, loss: 2.229600\n",
      "Epoch 185, loss: 2.222833\n",
      "Epoch 186, loss: 2.178338\n",
      "Epoch 187, loss: 2.193587\n",
      "Epoch 188, loss: 2.218807\n",
      "Epoch 189, loss: 2.203470\n",
      "Epoch 190, loss: 2.204454\n",
      "Epoch 191, loss: 2.216172\n",
      "Epoch 192, loss: 2.179730\n",
      "Epoch 193, loss: 2.205986\n",
      "Epoch 194, loss: 2.216469\n",
      "Epoch 195, loss: 2.205001\n",
      "Epoch 196, loss: 2.194767\n",
      "Epoch 197, loss: 2.188808\n",
      "Epoch 198, loss: 2.229836\n",
      "Epoch 199, loss: 2.195817\n",
      "Epoch 0, loss: 2.302757\n",
      "Epoch 1, loss: 2.301432\n",
      "Epoch 2, loss: 2.299395\n",
      "Epoch 3, loss: 2.299712\n",
      "Epoch 4, loss: 2.300412\n",
      "Epoch 5, loss: 2.297741\n",
      "Epoch 6, loss: 2.297404\n",
      "Epoch 7, loss: 2.295131\n",
      "Epoch 8, loss: 2.291710\n",
      "Epoch 9, loss: 2.294309\n",
      "Epoch 10, loss: 2.297269\n",
      "Epoch 11, loss: 2.295752\n",
      "Epoch 12, loss: 2.289085\n",
      "Epoch 13, loss: 2.293310\n",
      "Epoch 14, loss: 2.286796\n",
      "Epoch 15, loss: 2.284465\n",
      "Epoch 16, loss: 2.287174\n",
      "Epoch 17, loss: 2.287204\n",
      "Epoch 18, loss: 2.289105\n",
      "Epoch 19, loss: 2.279509\n",
      "Epoch 20, loss: 2.286220\n",
      "Epoch 21, loss: 2.284111\n",
      "Epoch 22, loss: 2.280016\n",
      "Epoch 23, loss: 2.285261\n",
      "Epoch 24, loss: 2.279224\n",
      "Epoch 25, loss: 2.282102\n",
      "Epoch 26, loss: 2.279715\n",
      "Epoch 27, loss: 2.283292\n",
      "Epoch 28, loss: 2.282148\n",
      "Epoch 29, loss: 2.272720\n",
      "Epoch 30, loss: 2.279138\n",
      "Epoch 31, loss: 2.279105\n",
      "Epoch 32, loss: 2.278807\n",
      "Epoch 33, loss: 2.279083\n",
      "Epoch 34, loss: 2.281264\n",
      "Epoch 35, loss: 2.277905\n",
      "Epoch 36, loss: 2.271946\n",
      "Epoch 37, loss: 2.279375\n",
      "Epoch 38, loss: 2.271260\n",
      "Epoch 39, loss: 2.269680\n",
      "Epoch 40, loss: 2.272639\n",
      "Epoch 41, loss: 2.268695\n",
      "Epoch 42, loss: 2.267485\n",
      "Epoch 43, loss: 2.269663\n",
      "Epoch 44, loss: 2.262298\n",
      "Epoch 45, loss: 2.264159\n",
      "Epoch 46, loss: 2.276484\n",
      "Epoch 47, loss: 2.272792\n",
      "Epoch 48, loss: 2.260142\n",
      "Epoch 49, loss: 2.261404\n",
      "Epoch 50, loss: 2.259333\n",
      "Epoch 51, loss: 2.253643\n",
      "Epoch 52, loss: 2.260066\n",
      "Epoch 53, loss: 2.268555\n",
      "Epoch 54, loss: 2.253440\n",
      "Epoch 55, loss: 2.245761\n",
      "Epoch 56, loss: 2.258434\n",
      "Epoch 57, loss: 2.262764\n",
      "Epoch 58, loss: 2.263476\n",
      "Epoch 59, loss: 2.256701\n",
      "Epoch 60, loss: 2.261247\n",
      "Epoch 61, loss: 2.266987\n",
      "Epoch 62, loss: 2.252017\n",
      "Epoch 63, loss: 2.250025\n",
      "Epoch 64, loss: 2.262952\n",
      "Epoch 65, loss: 2.250094\n",
      "Epoch 66, loss: 2.254000\n",
      "Epoch 67, loss: 2.264072\n",
      "Epoch 68, loss: 2.256628\n",
      "Epoch 69, loss: 2.248085\n",
      "Epoch 70, loss: 2.247558\n",
      "Epoch 71, loss: 2.256533\n",
      "Epoch 72, loss: 2.267081\n",
      "Epoch 73, loss: 2.248729\n",
      "Epoch 74, loss: 2.254586\n",
      "Epoch 75, loss: 2.269589\n",
      "Epoch 76, loss: 2.240176\n",
      "Epoch 77, loss: 2.244686\n",
      "Epoch 78, loss: 2.251290\n",
      "Epoch 79, loss: 2.248196\n",
      "Epoch 80, loss: 2.234375\n",
      "Epoch 81, loss: 2.250133\n",
      "Epoch 82, loss: 2.242799\n",
      "Epoch 83, loss: 2.245314\n",
      "Epoch 84, loss: 2.236505\n",
      "Epoch 85, loss: 2.246579\n",
      "Epoch 86, loss: 2.244908\n",
      "Epoch 87, loss: 2.234929\n",
      "Epoch 88, loss: 2.253255\n",
      "Epoch 89, loss: 2.233776\n",
      "Epoch 90, loss: 2.250923\n",
      "Epoch 91, loss: 2.228390\n",
      "Epoch 92, loss: 2.250085\n",
      "Epoch 93, loss: 2.245274\n",
      "Epoch 94, loss: 2.241133\n",
      "Epoch 95, loss: 2.248030\n",
      "Epoch 96, loss: 2.237224\n",
      "Epoch 97, loss: 2.244741\n",
      "Epoch 98, loss: 2.253814\n",
      "Epoch 99, loss: 2.233366\n",
      "Epoch 100, loss: 2.246266\n",
      "Epoch 101, loss: 2.241280\n",
      "Epoch 102, loss: 2.253457\n",
      "Epoch 103, loss: 2.252059\n",
      "Epoch 104, loss: 2.237966\n",
      "Epoch 105, loss: 2.238815\n",
      "Epoch 106, loss: 2.216265\n",
      "Epoch 107, loss: 2.247425\n",
      "Epoch 108, loss: 2.239010\n",
      "Epoch 109, loss: 2.241424\n",
      "Epoch 110, loss: 2.225515\n",
      "Epoch 111, loss: 2.240830\n",
      "Epoch 112, loss: 2.225064\n",
      "Epoch 113, loss: 2.234463\n",
      "Epoch 114, loss: 2.227842\n",
      "Epoch 115, loss: 2.209367\n",
      "Epoch 116, loss: 2.238662\n",
      "Epoch 117, loss: 2.218729\n",
      "Epoch 118, loss: 2.233303\n",
      "Epoch 119, loss: 2.229580\n",
      "Epoch 120, loss: 2.237892\n",
      "Epoch 121, loss: 2.224097\n",
      "Epoch 122, loss: 2.213373\n",
      "Epoch 123, loss: 2.214961\n",
      "Epoch 124, loss: 2.224985\n",
      "Epoch 125, loss: 2.216291\n",
      "Epoch 126, loss: 2.239344\n",
      "Epoch 127, loss: 2.223180\n",
      "Epoch 128, loss: 2.231314\n",
      "Epoch 129, loss: 2.217459\n",
      "Epoch 130, loss: 2.235901\n",
      "Epoch 131, loss: 2.236063\n",
      "Epoch 132, loss: 2.215533\n",
      "Epoch 133, loss: 2.216380\n",
      "Epoch 134, loss: 2.215758\n",
      "Epoch 135, loss: 2.203225\n",
      "Epoch 136, loss: 2.240708\n",
      "Epoch 137, loss: 2.217096\n",
      "Epoch 138, loss: 2.224308\n",
      "Epoch 139, loss: 2.229289\n",
      "Epoch 140, loss: 2.213374\n",
      "Epoch 141, loss: 2.221528\n",
      "Epoch 142, loss: 2.237182\n",
      "Epoch 143, loss: 2.214122\n",
      "Epoch 144, loss: 2.203051\n",
      "Epoch 145, loss: 2.224374\n",
      "Epoch 146, loss: 2.215684\n",
      "Epoch 147, loss: 2.208667\n",
      "Epoch 148, loss: 2.217730\n",
      "Epoch 149, loss: 2.225182\n",
      "Epoch 150, loss: 2.218159\n",
      "Epoch 151, loss: 2.216399\n",
      "Epoch 152, loss: 2.210451\n",
      "Epoch 153, loss: 2.210114\n",
      "Epoch 154, loss: 2.218592\n",
      "Epoch 155, loss: 2.224495\n",
      "Epoch 156, loss: 2.215611\n",
      "Epoch 157, loss: 2.223896\n",
      "Epoch 158, loss: 2.202900\n",
      "Epoch 159, loss: 2.216550\n",
      "Epoch 160, loss: 2.205124\n",
      "Epoch 161, loss: 2.209731\n",
      "Epoch 162, loss: 2.224130\n",
      "Epoch 163, loss: 2.211638\n",
      "Epoch 164, loss: 2.208134\n",
      "Epoch 165, loss: 2.234646\n",
      "Epoch 166, loss: 2.193706\n",
      "Epoch 167, loss: 2.213447\n",
      "Epoch 168, loss: 2.204057\n",
      "Epoch 169, loss: 2.219443\n",
      "Epoch 170, loss: 2.223973\n",
      "Epoch 171, loss: 2.205477\n",
      "Epoch 172, loss: 2.213108\n",
      "Epoch 173, loss: 2.212013\n",
      "Epoch 174, loss: 2.239442\n",
      "Epoch 175, loss: 2.219283\n",
      "Epoch 176, loss: 2.231849\n",
      "Epoch 177, loss: 2.232123\n",
      "Epoch 178, loss: 2.189870\n",
      "Epoch 179, loss: 2.208982\n",
      "Epoch 180, loss: 2.220751\n",
      "Epoch 181, loss: 2.216892\n",
      "Epoch 182, loss: 2.198661\n",
      "Epoch 183, loss: 2.212953\n",
      "Epoch 184, loss: 2.208977\n",
      "Epoch 185, loss: 2.224889\n",
      "Epoch 186, loss: 2.210479\n",
      "Epoch 187, loss: 2.207333\n",
      "Epoch 188, loss: 2.206215\n",
      "Epoch 189, loss: 2.188516\n",
      "Epoch 190, loss: 2.202432\n",
      "Epoch 191, loss: 2.204952\n",
      "Epoch 192, loss: 2.201690\n",
      "Epoch 193, loss: 2.192772\n",
      "Epoch 194, loss: 2.193102\n",
      "Epoch 195, loss: 2.201221\n",
      "Epoch 196, loss: 2.197109\n",
      "Epoch 197, loss: 2.196530\n",
      "Epoch 198, loss: 2.220137\n",
      "Epoch 199, loss: 2.211722\n",
      "Epoch 0, loss: 2.301556\n",
      "Epoch 1, loss: 2.298585\n",
      "Epoch 2, loss: 2.300562\n",
      "Epoch 3, loss: 2.298780\n",
      "Epoch 4, loss: 2.298210\n",
      "Epoch 5, loss: 2.296287\n",
      "Epoch 6, loss: 2.297744\n",
      "Epoch 7, loss: 2.297786\n",
      "Epoch 8, loss: 2.295659\n",
      "Epoch 9, loss: 2.294862\n",
      "Epoch 10, loss: 2.294820\n",
      "Epoch 11, loss: 2.298261\n",
      "Epoch 12, loss: 2.289706\n",
      "Epoch 13, loss: 2.286338\n",
      "Epoch 14, loss: 2.295141\n",
      "Epoch 15, loss: 2.286292\n",
      "Epoch 16, loss: 2.284095\n",
      "Epoch 17, loss: 2.280984\n",
      "Epoch 18, loss: 2.286966\n",
      "Epoch 19, loss: 2.287882\n",
      "Epoch 20, loss: 2.278953\n",
      "Epoch 21, loss: 2.288827\n",
      "Epoch 22, loss: 2.283088\n",
      "Epoch 23, loss: 2.283069\n",
      "Epoch 24, loss: 2.282517\n",
      "Epoch 25, loss: 2.279986\n",
      "Epoch 26, loss: 2.285652\n",
      "Epoch 27, loss: 2.284506\n",
      "Epoch 28, loss: 2.278609\n",
      "Epoch 29, loss: 2.282833\n",
      "Epoch 30, loss: 2.268369\n",
      "Epoch 31, loss: 2.273445\n",
      "Epoch 32, loss: 2.272125\n",
      "Epoch 33, loss: 2.277923\n",
      "Epoch 34, loss: 2.278297\n",
      "Epoch 35, loss: 2.273415\n",
      "Epoch 36, loss: 2.278060\n",
      "Epoch 37, loss: 2.268663\n",
      "Epoch 38, loss: 2.278641\n",
      "Epoch 39, loss: 2.263045\n",
      "Epoch 40, loss: 2.269426\n",
      "Epoch 41, loss: 2.261469\n",
      "Epoch 42, loss: 2.267409\n",
      "Epoch 43, loss: 2.276204\n",
      "Epoch 44, loss: 2.265268\n",
      "Epoch 45, loss: 2.261943\n",
      "Epoch 46, loss: 2.267088\n",
      "Epoch 47, loss: 2.264659\n",
      "Epoch 48, loss: 2.262027\n",
      "Epoch 49, loss: 2.265587\n",
      "Epoch 50, loss: 2.255664\n",
      "Epoch 51, loss: 2.267675\n",
      "Epoch 52, loss: 2.267785\n",
      "Epoch 53, loss: 2.269703\n",
      "Epoch 54, loss: 2.261133\n",
      "Epoch 55, loss: 2.255685\n",
      "Epoch 56, loss: 2.254168\n",
      "Epoch 57, loss: 2.247733\n",
      "Epoch 58, loss: 2.256009\n",
      "Epoch 59, loss: 2.261492\n",
      "Epoch 60, loss: 2.266169\n",
      "Epoch 61, loss: 2.257709\n",
      "Epoch 62, loss: 2.258625\n",
      "Epoch 63, loss: 2.250206\n",
      "Epoch 64, loss: 2.256646\n",
      "Epoch 65, loss: 2.268458\n",
      "Epoch 66, loss: 2.268632\n",
      "Epoch 67, loss: 2.257372\n",
      "Epoch 68, loss: 2.254349\n",
      "Epoch 69, loss: 2.242541\n",
      "Epoch 70, loss: 2.250042\n",
      "Epoch 71, loss: 2.268535\n",
      "Epoch 72, loss: 2.247233\n",
      "Epoch 73, loss: 2.268211\n",
      "Epoch 74, loss: 2.258516\n",
      "Epoch 75, loss: 2.257872\n",
      "Epoch 76, loss: 2.263775\n",
      "Epoch 77, loss: 2.238370\n",
      "Epoch 78, loss: 2.244799\n",
      "Epoch 79, loss: 2.249660\n",
      "Epoch 80, loss: 2.239271\n",
      "Epoch 81, loss: 2.239572\n",
      "Epoch 82, loss: 2.245828\n",
      "Epoch 83, loss: 2.249532\n",
      "Epoch 84, loss: 2.233386\n",
      "Epoch 85, loss: 2.241943\n",
      "Epoch 86, loss: 2.246630\n",
      "Epoch 87, loss: 2.245085\n",
      "Epoch 88, loss: 2.240928\n",
      "Epoch 89, loss: 2.249644\n",
      "Epoch 90, loss: 2.250558\n",
      "Epoch 91, loss: 2.240123\n",
      "Epoch 92, loss: 2.219718\n",
      "Epoch 93, loss: 2.239272\n",
      "Epoch 94, loss: 2.247834\n",
      "Epoch 95, loss: 2.236011\n",
      "Epoch 96, loss: 2.220323\n",
      "Epoch 97, loss: 2.227742\n",
      "Epoch 98, loss: 2.245895\n",
      "Epoch 99, loss: 2.231486\n",
      "Epoch 100, loss: 2.230421\n",
      "Epoch 101, loss: 2.242508\n",
      "Epoch 102, loss: 2.226704\n",
      "Epoch 103, loss: 2.225046\n",
      "Epoch 104, loss: 2.231341\n",
      "Epoch 105, loss: 2.240444\n",
      "Epoch 106, loss: 2.244193\n",
      "Epoch 107, loss: 2.245232\n",
      "Epoch 108, loss: 2.224177\n",
      "Epoch 109, loss: 2.215686\n",
      "Epoch 110, loss: 2.213893\n",
      "Epoch 111, loss: 2.231955\n",
      "Epoch 112, loss: 2.234266\n",
      "Epoch 113, loss: 2.237107\n",
      "Epoch 114, loss: 2.229869\n",
      "Epoch 115, loss: 2.218099\n",
      "Epoch 116, loss: 2.230799\n",
      "Epoch 117, loss: 2.233919\n",
      "Epoch 118, loss: 2.232089\n",
      "Epoch 119, loss: 2.240263\n",
      "Epoch 120, loss: 2.208141\n",
      "Epoch 121, loss: 2.241503\n",
      "Epoch 122, loss: 2.221180\n",
      "Epoch 123, loss: 2.243628\n",
      "Epoch 124, loss: 2.213264\n",
      "Epoch 125, loss: 2.216510\n",
      "Epoch 126, loss: 2.223385\n",
      "Epoch 127, loss: 2.228633\n",
      "Epoch 128, loss: 2.209744\n",
      "Epoch 129, loss: 2.213172\n",
      "Epoch 130, loss: 2.220055\n",
      "Epoch 131, loss: 2.212108\n",
      "Epoch 132, loss: 2.218818\n",
      "Epoch 133, loss: 2.204295\n",
      "Epoch 134, loss: 2.241948\n",
      "Epoch 135, loss: 2.222278\n",
      "Epoch 136, loss: 2.233529\n",
      "Epoch 137, loss: 2.222266\n",
      "Epoch 138, loss: 2.206095\n",
      "Epoch 139, loss: 2.230988\n",
      "Epoch 140, loss: 2.213006\n",
      "Epoch 141, loss: 2.187232\n",
      "Epoch 142, loss: 2.223845\n",
      "Epoch 143, loss: 2.242560\n",
      "Epoch 144, loss: 2.235505\n",
      "Epoch 145, loss: 2.198455\n",
      "Epoch 146, loss: 2.207364\n",
      "Epoch 147, loss: 2.221012\n",
      "Epoch 148, loss: 2.202237\n",
      "Epoch 149, loss: 2.227171\n",
      "Epoch 150, loss: 2.201816\n",
      "Epoch 151, loss: 2.217541\n",
      "Epoch 152, loss: 2.220382\n",
      "Epoch 153, loss: 2.222654\n",
      "Epoch 154, loss: 2.204679\n",
      "Epoch 155, loss: 2.222386\n",
      "Epoch 156, loss: 2.204049\n",
      "Epoch 157, loss: 2.218702\n",
      "Epoch 158, loss: 2.241845\n",
      "Epoch 159, loss: 2.188518\n",
      "Epoch 160, loss: 2.208131\n",
      "Epoch 161, loss: 2.189048\n",
      "Epoch 162, loss: 2.202935\n",
      "Epoch 163, loss: 2.229428\n",
      "Epoch 164, loss: 2.224192\n",
      "Epoch 165, loss: 2.220177\n",
      "Epoch 166, loss: 2.193358\n",
      "Epoch 167, loss: 2.218851\n",
      "Epoch 168, loss: 2.200192\n",
      "Epoch 169, loss: 2.198322\n",
      "Epoch 170, loss: 2.216089\n",
      "Epoch 171, loss: 2.208955\n",
      "Epoch 172, loss: 2.220474\n",
      "Epoch 173, loss: 2.193053\n",
      "Epoch 174, loss: 2.200134\n",
      "Epoch 175, loss: 2.217964\n",
      "Epoch 176, loss: 2.212540\n",
      "Epoch 177, loss: 2.213173\n",
      "Epoch 178, loss: 2.196474\n",
      "Epoch 179, loss: 2.227355\n",
      "Epoch 180, loss: 2.218458\n",
      "Epoch 181, loss: 2.203014\n",
      "Epoch 182, loss: 2.201246\n",
      "Epoch 183, loss: 2.206740\n",
      "Epoch 184, loss: 2.204469\n",
      "Epoch 185, loss: 2.231195\n",
      "Epoch 186, loss: 2.200313\n",
      "Epoch 187, loss: 2.201449\n",
      "Epoch 188, loss: 2.235688\n",
      "Epoch 189, loss: 2.211563\n",
      "Epoch 190, loss: 2.189465\n",
      "Epoch 191, loss: 2.200158\n",
      "Epoch 192, loss: 2.196801\n",
      "Epoch 193, loss: 2.220479\n",
      "Epoch 194, loss: 2.201099\n",
      "Epoch 195, loss: 2.199448\n",
      "Epoch 196, loss: 2.206642\n",
      "Epoch 197, loss: 2.213570\n",
      "Epoch 198, loss: 2.171408\n",
      "Epoch 199, loss: 2.168673\n",
      "Epoch 0, loss: 2.302916\n",
      "Epoch 1, loss: 2.303180\n",
      "Epoch 2, loss: 2.301847\n",
      "Epoch 3, loss: 2.302690\n",
      "Epoch 4, loss: 2.302607\n",
      "Epoch 5, loss: 2.301783\n",
      "Epoch 6, loss: 2.302788\n",
      "Epoch 7, loss: 2.301873\n",
      "Epoch 8, loss: 2.302622\n",
      "Epoch 9, loss: 2.303238\n",
      "Epoch 10, loss: 2.302849\n",
      "Epoch 11, loss: 2.300503\n",
      "Epoch 12, loss: 2.300984\n",
      "Epoch 13, loss: 2.302147\n",
      "Epoch 14, loss: 2.301492\n",
      "Epoch 15, loss: 2.300733\n",
      "Epoch 16, loss: 2.300793\n",
      "Epoch 17, loss: 2.300768\n",
      "Epoch 18, loss: 2.301092\n",
      "Epoch 19, loss: 2.301398\n",
      "Epoch 20, loss: 2.300095\n",
      "Epoch 21, loss: 2.301367\n",
      "Epoch 22, loss: 2.300405\n",
      "Epoch 23, loss: 2.301138\n",
      "Epoch 24, loss: 2.299583\n",
      "Epoch 25, loss: 2.301455\n",
      "Epoch 26, loss: 2.299206\n",
      "Epoch 27, loss: 2.298786\n",
      "Epoch 28, loss: 2.300864\n",
      "Epoch 29, loss: 2.301643\n",
      "Epoch 30, loss: 2.299812\n",
      "Epoch 31, loss: 2.299331\n",
      "Epoch 32, loss: 2.300410\n",
      "Epoch 33, loss: 2.299495\n",
      "Epoch 34, loss: 2.298332\n",
      "Epoch 35, loss: 2.300084\n",
      "Epoch 36, loss: 2.299395\n",
      "Epoch 37, loss: 2.296451\n",
      "Epoch 38, loss: 2.298267\n",
      "Epoch 39, loss: 2.299181\n",
      "Epoch 40, loss: 2.299138\n",
      "Epoch 41, loss: 2.298026\n",
      "Epoch 42, loss: 2.298610\n",
      "Epoch 43, loss: 2.298590\n",
      "Epoch 44, loss: 2.299578\n",
      "Epoch 45, loss: 2.298402\n",
      "Epoch 46, loss: 2.298091\n",
      "Epoch 47, loss: 2.300377\n",
      "Epoch 48, loss: 2.297326\n",
      "Epoch 49, loss: 2.297723\n",
      "Epoch 50, loss: 2.297677\n",
      "Epoch 51, loss: 2.298002\n",
      "Epoch 52, loss: 2.298567\n",
      "Epoch 53, loss: 2.295905\n",
      "Epoch 54, loss: 2.299360\n",
      "Epoch 55, loss: 2.296266\n",
      "Epoch 56, loss: 2.296568\n",
      "Epoch 57, loss: 2.296657\n",
      "Epoch 58, loss: 2.296870\n",
      "Epoch 59, loss: 2.297438\n",
      "Epoch 60, loss: 2.295293\n",
      "Epoch 61, loss: 2.294572\n",
      "Epoch 62, loss: 2.297685\n",
      "Epoch 63, loss: 2.295410\n",
      "Epoch 64, loss: 2.292231\n",
      "Epoch 65, loss: 2.298290\n",
      "Epoch 66, loss: 2.294884\n",
      "Epoch 67, loss: 2.297990\n",
      "Epoch 68, loss: 2.297283\n",
      "Epoch 69, loss: 2.296436\n",
      "Epoch 70, loss: 2.296506\n",
      "Epoch 71, loss: 2.299584\n",
      "Epoch 72, loss: 2.294679\n",
      "Epoch 73, loss: 2.298096\n",
      "Epoch 74, loss: 2.295938\n",
      "Epoch 75, loss: 2.294367\n",
      "Epoch 76, loss: 2.296949\n",
      "Epoch 77, loss: 2.295731\n",
      "Epoch 78, loss: 2.298326\n",
      "Epoch 79, loss: 2.292681\n",
      "Epoch 80, loss: 2.294576\n",
      "Epoch 81, loss: 2.293179\n",
      "Epoch 82, loss: 2.293328\n",
      "Epoch 83, loss: 2.295649\n",
      "Epoch 84, loss: 2.296719\n",
      "Epoch 85, loss: 2.294024\n",
      "Epoch 86, loss: 2.295769\n",
      "Epoch 87, loss: 2.293307\n",
      "Epoch 88, loss: 2.292060\n",
      "Epoch 89, loss: 2.292852\n",
      "Epoch 90, loss: 2.297055\n",
      "Epoch 91, loss: 2.297931\n",
      "Epoch 92, loss: 2.295805\n",
      "Epoch 93, loss: 2.296005\n",
      "Epoch 94, loss: 2.295313\n",
      "Epoch 95, loss: 2.296594\n",
      "Epoch 96, loss: 2.295440\n",
      "Epoch 97, loss: 2.296904\n",
      "Epoch 98, loss: 2.292176\n",
      "Epoch 99, loss: 2.291780\n",
      "Epoch 100, loss: 2.291861\n",
      "Epoch 101, loss: 2.296897\n",
      "Epoch 102, loss: 2.296742\n",
      "Epoch 103, loss: 2.291787\n",
      "Epoch 104, loss: 2.292367\n",
      "Epoch 105, loss: 2.291473\n",
      "Epoch 106, loss: 2.293276\n",
      "Epoch 107, loss: 2.288400\n",
      "Epoch 108, loss: 2.291422\n",
      "Epoch 109, loss: 2.290644\n",
      "Epoch 110, loss: 2.290370\n",
      "Epoch 111, loss: 2.295995\n",
      "Epoch 112, loss: 2.293529\n",
      "Epoch 113, loss: 2.290249\n",
      "Epoch 114, loss: 2.292905\n",
      "Epoch 115, loss: 2.288209\n",
      "Epoch 116, loss: 2.296461\n",
      "Epoch 117, loss: 2.292583\n",
      "Epoch 118, loss: 2.295605\n",
      "Epoch 119, loss: 2.294371\n",
      "Epoch 120, loss: 2.292028\n",
      "Epoch 121, loss: 2.288433\n",
      "Epoch 122, loss: 2.291421\n",
      "Epoch 123, loss: 2.285535\n",
      "Epoch 124, loss: 2.292111\n",
      "Epoch 125, loss: 2.289318\n",
      "Epoch 126, loss: 2.287403\n",
      "Epoch 127, loss: 2.290810\n",
      "Epoch 128, loss: 2.294054\n",
      "Epoch 129, loss: 2.292730\n",
      "Epoch 130, loss: 2.290243\n",
      "Epoch 131, loss: 2.297880\n",
      "Epoch 132, loss: 2.286868\n",
      "Epoch 133, loss: 2.289372\n",
      "Epoch 134, loss: 2.291860\n",
      "Epoch 135, loss: 2.288183\n",
      "Epoch 136, loss: 2.292253\n",
      "Epoch 137, loss: 2.291752\n",
      "Epoch 138, loss: 2.294825\n",
      "Epoch 139, loss: 2.292806\n",
      "Epoch 140, loss: 2.293595\n",
      "Epoch 141, loss: 2.288143\n",
      "Epoch 142, loss: 2.289079\n",
      "Epoch 143, loss: 2.295667\n",
      "Epoch 144, loss: 2.293482\n",
      "Epoch 145, loss: 2.285006\n",
      "Epoch 146, loss: 2.288980\n",
      "Epoch 147, loss: 2.291478\n",
      "Epoch 148, loss: 2.289620\n",
      "Epoch 149, loss: 2.291581\n",
      "Epoch 150, loss: 2.289958\n",
      "Epoch 151, loss: 2.291111\n",
      "Epoch 152, loss: 2.289972\n",
      "Epoch 153, loss: 2.285858\n",
      "Epoch 154, loss: 2.292715\n",
      "Epoch 155, loss: 2.284024\n",
      "Epoch 156, loss: 2.285947\n",
      "Epoch 157, loss: 2.289317\n",
      "Epoch 158, loss: 2.289223\n",
      "Epoch 159, loss: 2.285573\n",
      "Epoch 160, loss: 2.290815\n",
      "Epoch 161, loss: 2.291893\n",
      "Epoch 162, loss: 2.288898\n",
      "Epoch 163, loss: 2.286625\n",
      "Epoch 164, loss: 2.287376\n",
      "Epoch 165, loss: 2.288021\n",
      "Epoch 166, loss: 2.287449\n",
      "Epoch 167, loss: 2.287547\n",
      "Epoch 168, loss: 2.291383\n",
      "Epoch 169, loss: 2.282955\n",
      "Epoch 170, loss: 2.287456\n",
      "Epoch 171, loss: 2.285786\n",
      "Epoch 172, loss: 2.286770\n",
      "Epoch 173, loss: 2.283377\n",
      "Epoch 174, loss: 2.288613\n",
      "Epoch 175, loss: 2.285128\n",
      "Epoch 176, loss: 2.284598\n",
      "Epoch 177, loss: 2.289074\n",
      "Epoch 178, loss: 2.286001\n",
      "Epoch 179, loss: 2.282042\n",
      "Epoch 180, loss: 2.286905\n",
      "Epoch 181, loss: 2.286281\n",
      "Epoch 182, loss: 2.289138\n",
      "Epoch 183, loss: 2.287884\n",
      "Epoch 184, loss: 2.283574\n",
      "Epoch 185, loss: 2.286687\n",
      "Epoch 186, loss: 2.286886\n",
      "Epoch 187, loss: 2.289482\n",
      "Epoch 188, loss: 2.289702\n",
      "Epoch 189, loss: 2.283711\n",
      "Epoch 190, loss: 2.287566\n",
      "Epoch 191, loss: 2.287093\n",
      "Epoch 192, loss: 2.288083\n",
      "Epoch 193, loss: 2.283354\n",
      "Epoch 194, loss: 2.287483\n",
      "Epoch 195, loss: 2.284426\n",
      "Epoch 196, loss: 2.285954\n",
      "Epoch 197, loss: 2.282708\n",
      "Epoch 198, loss: 2.286985\n",
      "Epoch 199, loss: 2.281904\n",
      "Epoch 0, loss: 2.302733\n",
      "Epoch 1, loss: 2.301945\n",
      "Epoch 2, loss: 2.301974\n",
      "Epoch 3, loss: 2.301403\n",
      "Epoch 4, loss: 2.301238\n",
      "Epoch 5, loss: 2.301334\n",
      "Epoch 6, loss: 2.303029\n",
      "Epoch 7, loss: 2.301956\n",
      "Epoch 8, loss: 2.299944\n",
      "Epoch 9, loss: 2.301706\n",
      "Epoch 10, loss: 2.301947\n",
      "Epoch 11, loss: 2.301110\n",
      "Epoch 12, loss: 2.301352\n",
      "Epoch 13, loss: 2.299842\n",
      "Epoch 14, loss: 2.300187\n",
      "Epoch 15, loss: 2.300663\n",
      "Epoch 16, loss: 2.302016\n",
      "Epoch 17, loss: 2.299089\n",
      "Epoch 18, loss: 2.301499\n",
      "Epoch 19, loss: 2.299228\n",
      "Epoch 20, loss: 2.299999\n",
      "Epoch 21, loss: 2.297310\n",
      "Epoch 22, loss: 2.298764\n",
      "Epoch 23, loss: 2.300899\n",
      "Epoch 24, loss: 2.301049\n",
      "Epoch 25, loss: 2.297936\n",
      "Epoch 26, loss: 2.299917\n",
      "Epoch 27, loss: 2.299159\n",
      "Epoch 28, loss: 2.299505\n",
      "Epoch 29, loss: 2.299333\n",
      "Epoch 30, loss: 2.297903\n",
      "Epoch 31, loss: 2.298236\n",
      "Epoch 32, loss: 2.299408\n",
      "Epoch 33, loss: 2.298122\n",
      "Epoch 34, loss: 2.299994\n",
      "Epoch 35, loss: 2.298167\n",
      "Epoch 36, loss: 2.299624\n",
      "Epoch 37, loss: 2.298154\n",
      "Epoch 38, loss: 2.298324\n",
      "Epoch 39, loss: 2.300067\n",
      "Epoch 40, loss: 2.299020\n",
      "Epoch 41, loss: 2.298481\n",
      "Epoch 42, loss: 2.297950\n",
      "Epoch 43, loss: 2.296859\n",
      "Epoch 44, loss: 2.297398\n",
      "Epoch 45, loss: 2.296470\n",
      "Epoch 46, loss: 2.297720\n",
      "Epoch 47, loss: 2.298482\n",
      "Epoch 48, loss: 2.296469\n",
      "Epoch 49, loss: 2.295598\n",
      "Epoch 50, loss: 2.297352\n",
      "Epoch 51, loss: 2.296447\n",
      "Epoch 52, loss: 2.295345\n",
      "Epoch 53, loss: 2.297631\n",
      "Epoch 54, loss: 2.296236\n",
      "Epoch 55, loss: 2.296358\n",
      "Epoch 56, loss: 2.298301\n",
      "Epoch 57, loss: 2.299618\n",
      "Epoch 58, loss: 2.295060\n",
      "Epoch 59, loss: 2.297252\n",
      "Epoch 60, loss: 2.295989\n",
      "Epoch 61, loss: 2.297244\n",
      "Epoch 62, loss: 2.298112\n",
      "Epoch 63, loss: 2.295434\n",
      "Epoch 64, loss: 2.297154\n",
      "Epoch 65, loss: 2.295331\n",
      "Epoch 66, loss: 2.295381\n",
      "Epoch 67, loss: 2.296175\n",
      "Epoch 68, loss: 2.297898\n",
      "Epoch 69, loss: 2.297615\n",
      "Epoch 70, loss: 2.296098\n",
      "Epoch 71, loss: 2.294989\n",
      "Epoch 72, loss: 2.294876\n",
      "Epoch 73, loss: 2.293654\n",
      "Epoch 74, loss: 2.294777\n",
      "Epoch 75, loss: 2.295883\n",
      "Epoch 76, loss: 2.294896\n",
      "Epoch 77, loss: 2.293047\n",
      "Epoch 78, loss: 2.291381\n",
      "Epoch 79, loss: 2.296735\n",
      "Epoch 80, loss: 2.293490\n",
      "Epoch 81, loss: 2.295710\n",
      "Epoch 82, loss: 2.296444\n",
      "Epoch 83, loss: 2.294577\n",
      "Epoch 84, loss: 2.294786\n",
      "Epoch 85, loss: 2.294548\n",
      "Epoch 86, loss: 2.293041\n",
      "Epoch 87, loss: 2.292164\n",
      "Epoch 88, loss: 2.293687\n",
      "Epoch 89, loss: 2.297088\n",
      "Epoch 90, loss: 2.293032\n",
      "Epoch 91, loss: 2.295221\n",
      "Epoch 92, loss: 2.291935\n",
      "Epoch 93, loss: 2.295066\n",
      "Epoch 94, loss: 2.293267\n",
      "Epoch 95, loss: 2.289445\n",
      "Epoch 96, loss: 2.291511\n",
      "Epoch 97, loss: 2.294072\n",
      "Epoch 98, loss: 2.291215\n",
      "Epoch 99, loss: 2.293967\n",
      "Epoch 100, loss: 2.290303\n",
      "Epoch 101, loss: 2.292847\n",
      "Epoch 102, loss: 2.290917\n",
      "Epoch 103, loss: 2.292613\n",
      "Epoch 104, loss: 2.293504\n",
      "Epoch 105, loss: 2.291839\n",
      "Epoch 106, loss: 2.289583\n",
      "Epoch 107, loss: 2.290424\n",
      "Epoch 108, loss: 2.290728\n",
      "Epoch 109, loss: 2.294891\n",
      "Epoch 110, loss: 2.288782\n",
      "Epoch 111, loss: 2.294598\n",
      "Epoch 112, loss: 2.287929\n",
      "Epoch 113, loss: 2.290111\n",
      "Epoch 114, loss: 2.290780\n",
      "Epoch 115, loss: 2.288625\n",
      "Epoch 116, loss: 2.291286\n",
      "Epoch 117, loss: 2.287709\n",
      "Epoch 118, loss: 2.294292\n",
      "Epoch 119, loss: 2.290118\n",
      "Epoch 120, loss: 2.290770\n",
      "Epoch 121, loss: 2.287370\n",
      "Epoch 122, loss: 2.287685\n",
      "Epoch 123, loss: 2.292304\n",
      "Epoch 124, loss: 2.286673\n",
      "Epoch 125, loss: 2.289752\n",
      "Epoch 126, loss: 2.286044\n",
      "Epoch 127, loss: 2.289124\n",
      "Epoch 128, loss: 2.288512\n",
      "Epoch 129, loss: 2.287361\n",
      "Epoch 130, loss: 2.292833\n",
      "Epoch 131, loss: 2.290609\n",
      "Epoch 132, loss: 2.289085\n",
      "Epoch 133, loss: 2.288057\n",
      "Epoch 134, loss: 2.290743\n",
      "Epoch 135, loss: 2.290579\n",
      "Epoch 136, loss: 2.291378\n",
      "Epoch 137, loss: 2.291327\n",
      "Epoch 138, loss: 2.288345\n",
      "Epoch 139, loss: 2.287558\n",
      "Epoch 140, loss: 2.288969\n",
      "Epoch 141, loss: 2.289311\n",
      "Epoch 142, loss: 2.288367\n",
      "Epoch 143, loss: 2.287996\n",
      "Epoch 144, loss: 2.291065\n",
      "Epoch 145, loss: 2.290508\n",
      "Epoch 146, loss: 2.287623\n",
      "Epoch 147, loss: 2.288884\n",
      "Epoch 148, loss: 2.297461\n",
      "Epoch 149, loss: 2.288160\n",
      "Epoch 150, loss: 2.294472\n",
      "Epoch 151, loss: 2.288383\n",
      "Epoch 152, loss: 2.288261\n",
      "Epoch 153, loss: 2.288113\n",
      "Epoch 154, loss: 2.290747\n",
      "Epoch 155, loss: 2.293346\n",
      "Epoch 156, loss: 2.289032\n",
      "Epoch 157, loss: 2.287207\n",
      "Epoch 158, loss: 2.286803\n",
      "Epoch 159, loss: 2.287778\n",
      "Epoch 160, loss: 2.284669\n",
      "Epoch 161, loss: 2.287454\n",
      "Epoch 162, loss: 2.290918\n",
      "Epoch 163, loss: 2.289549\n",
      "Epoch 164, loss: 2.295185\n",
      "Epoch 165, loss: 2.290997\n",
      "Epoch 166, loss: 2.288647\n",
      "Epoch 167, loss: 2.288387\n",
      "Epoch 168, loss: 2.291815\n",
      "Epoch 169, loss: 2.286402\n",
      "Epoch 170, loss: 2.285238\n",
      "Epoch 171, loss: 2.285132\n",
      "Epoch 172, loss: 2.286027\n",
      "Epoch 173, loss: 2.288764\n",
      "Epoch 174, loss: 2.282473\n",
      "Epoch 175, loss: 2.282323\n",
      "Epoch 176, loss: 2.286576\n",
      "Epoch 177, loss: 2.285559\n",
      "Epoch 178, loss: 2.286312\n",
      "Epoch 179, loss: 2.287160\n",
      "Epoch 180, loss: 2.283040\n",
      "Epoch 181, loss: 2.288769\n",
      "Epoch 182, loss: 2.284673\n",
      "Epoch 183, loss: 2.279812\n",
      "Epoch 184, loss: 2.282168\n",
      "Epoch 185, loss: 2.281001\n",
      "Epoch 186, loss: 2.287845\n",
      "Epoch 187, loss: 2.290384\n",
      "Epoch 188, loss: 2.279886\n",
      "Epoch 189, loss: 2.282188\n",
      "Epoch 190, loss: 2.285009\n",
      "Epoch 191, loss: 2.288389\n",
      "Epoch 192, loss: 2.279436\n",
      "Epoch 193, loss: 2.287822\n",
      "Epoch 194, loss: 2.283474\n",
      "Epoch 195, loss: 2.285226\n",
      "Epoch 196, loss: 2.288122\n",
      "Epoch 197, loss: 2.284371\n",
      "Epoch 198, loss: 2.281847\n",
      "Epoch 199, loss: 2.280797\n",
      "Epoch 0, loss: 2.302858\n",
      "Epoch 1, loss: 2.302116\n",
      "Epoch 2, loss: 2.301595\n",
      "Epoch 3, loss: 2.302313\n",
      "Epoch 4, loss: 2.300649\n",
      "Epoch 5, loss: 2.302140\n",
      "Epoch 6, loss: 2.300764\n",
      "Epoch 7, loss: 2.300700\n",
      "Epoch 8, loss: 2.300795\n",
      "Epoch 9, loss: 2.300750\n",
      "Epoch 10, loss: 2.300610\n",
      "Epoch 11, loss: 2.301794\n",
      "Epoch 12, loss: 2.300744\n",
      "Epoch 13, loss: 2.301040\n",
      "Epoch 14, loss: 2.300547\n",
      "Epoch 15, loss: 2.299513\n",
      "Epoch 16, loss: 2.300968\n",
      "Epoch 17, loss: 2.301226\n",
      "Epoch 18, loss: 2.300272\n",
      "Epoch 19, loss: 2.300174\n",
      "Epoch 20, loss: 2.301320\n",
      "Epoch 21, loss: 2.300979\n",
      "Epoch 22, loss: 2.299933\n",
      "Epoch 23, loss: 2.300451\n",
      "Epoch 24, loss: 2.299761\n",
      "Epoch 25, loss: 2.300368\n",
      "Epoch 26, loss: 2.299842\n",
      "Epoch 27, loss: 2.299367\n",
      "Epoch 28, loss: 2.298267\n",
      "Epoch 29, loss: 2.299393\n",
      "Epoch 30, loss: 2.300890\n",
      "Epoch 31, loss: 2.299223\n",
      "Epoch 32, loss: 2.298660\n",
      "Epoch 33, loss: 2.299580\n",
      "Epoch 34, loss: 2.299076\n",
      "Epoch 35, loss: 2.298965\n",
      "Epoch 36, loss: 2.301079\n",
      "Epoch 37, loss: 2.297671\n",
      "Epoch 38, loss: 2.299648\n",
      "Epoch 39, loss: 2.300937\n",
      "Epoch 40, loss: 2.297766\n",
      "Epoch 41, loss: 2.298315\n",
      "Epoch 42, loss: 2.298741\n",
      "Epoch 43, loss: 2.296184\n",
      "Epoch 44, loss: 2.297564\n",
      "Epoch 45, loss: 2.295191\n",
      "Epoch 46, loss: 2.299270\n",
      "Epoch 47, loss: 2.297338\n",
      "Epoch 48, loss: 2.296537\n",
      "Epoch 49, loss: 2.296944\n",
      "Epoch 50, loss: 2.299232\n",
      "Epoch 51, loss: 2.296722\n",
      "Epoch 52, loss: 2.297463\n",
      "Epoch 53, loss: 2.295315\n",
      "Epoch 54, loss: 2.295130\n",
      "Epoch 55, loss: 2.299425\n",
      "Epoch 56, loss: 2.298030\n",
      "Epoch 57, loss: 2.296924\n",
      "Epoch 58, loss: 2.297704\n",
      "Epoch 59, loss: 2.294964\n",
      "Epoch 60, loss: 2.297635\n",
      "Epoch 61, loss: 2.298465\n",
      "Epoch 62, loss: 2.297703\n",
      "Epoch 63, loss: 2.296510\n",
      "Epoch 64, loss: 2.296630\n",
      "Epoch 65, loss: 2.296003\n",
      "Epoch 66, loss: 2.295831\n",
      "Epoch 67, loss: 2.294005\n",
      "Epoch 68, loss: 2.294747\n",
      "Epoch 69, loss: 2.293978\n",
      "Epoch 70, loss: 2.294701\n",
      "Epoch 71, loss: 2.295070\n",
      "Epoch 72, loss: 2.294987\n",
      "Epoch 73, loss: 2.297153\n",
      "Epoch 74, loss: 2.295190\n",
      "Epoch 75, loss: 2.293836\n",
      "Epoch 76, loss: 2.292582\n",
      "Epoch 77, loss: 2.293775\n",
      "Epoch 78, loss: 2.295906\n",
      "Epoch 79, loss: 2.296741\n",
      "Epoch 80, loss: 2.293563\n",
      "Epoch 81, loss: 2.294172\n",
      "Epoch 82, loss: 2.291161\n",
      "Epoch 83, loss: 2.295433\n",
      "Epoch 84, loss: 2.294097\n",
      "Epoch 85, loss: 2.297384\n",
      "Epoch 86, loss: 2.295347\n",
      "Epoch 87, loss: 2.293473\n",
      "Epoch 88, loss: 2.291992\n",
      "Epoch 89, loss: 2.291293\n",
      "Epoch 90, loss: 2.291714\n",
      "Epoch 91, loss: 2.293301\n",
      "Epoch 92, loss: 2.292536\n",
      "Epoch 93, loss: 2.294276\n",
      "Epoch 94, loss: 2.289704\n",
      "Epoch 95, loss: 2.290797\n",
      "Epoch 96, loss: 2.293625\n",
      "Epoch 97, loss: 2.296266\n",
      "Epoch 98, loss: 2.289723\n",
      "Epoch 99, loss: 2.295111\n",
      "Epoch 100, loss: 2.292738\n",
      "Epoch 101, loss: 2.295356\n",
      "Epoch 102, loss: 2.295366\n",
      "Epoch 103, loss: 2.292577\n",
      "Epoch 104, loss: 2.291627\n",
      "Epoch 105, loss: 2.294189\n",
      "Epoch 106, loss: 2.293474\n",
      "Epoch 107, loss: 2.291858\n",
      "Epoch 108, loss: 2.290634\n",
      "Epoch 109, loss: 2.294373\n",
      "Epoch 110, loss: 2.291534\n",
      "Epoch 111, loss: 2.293320\n",
      "Epoch 112, loss: 2.288651\n",
      "Epoch 113, loss: 2.286858\n",
      "Epoch 114, loss: 2.294522\n",
      "Epoch 115, loss: 2.291925\n",
      "Epoch 116, loss: 2.289211\n",
      "Epoch 117, loss: 2.292174\n",
      "Epoch 118, loss: 2.286402\n",
      "Epoch 119, loss: 2.295253\n",
      "Epoch 120, loss: 2.292236\n",
      "Epoch 121, loss: 2.293510\n",
      "Epoch 122, loss: 2.288709\n",
      "Epoch 123, loss: 2.291016\n",
      "Epoch 124, loss: 2.291711\n",
      "Epoch 125, loss: 2.288429\n",
      "Epoch 126, loss: 2.294308\n",
      "Epoch 127, loss: 2.294844\n",
      "Epoch 128, loss: 2.290459\n",
      "Epoch 129, loss: 2.290287\n",
      "Epoch 130, loss: 2.287646\n",
      "Epoch 131, loss: 2.291244\n",
      "Epoch 132, loss: 2.293653\n",
      "Epoch 133, loss: 2.289356\n",
      "Epoch 134, loss: 2.292307\n",
      "Epoch 135, loss: 2.291711\n",
      "Epoch 136, loss: 2.292903\n",
      "Epoch 137, loss: 2.292902\n",
      "Epoch 138, loss: 2.288015\n",
      "Epoch 139, loss: 2.291590\n",
      "Epoch 140, loss: 2.288042\n",
      "Epoch 141, loss: 2.293111\n",
      "Epoch 142, loss: 2.290763\n",
      "Epoch 143, loss: 2.291542\n",
      "Epoch 144, loss: 2.288907\n",
      "Epoch 145, loss: 2.289475\n",
      "Epoch 146, loss: 2.291602\n",
      "Epoch 147, loss: 2.286759\n",
      "Epoch 148, loss: 2.287607\n",
      "Epoch 149, loss: 2.290931\n",
      "Epoch 150, loss: 2.287189\n",
      "Epoch 151, loss: 2.288388\n",
      "Epoch 152, loss: 2.289971\n",
      "Epoch 153, loss: 2.287084\n",
      "Epoch 154, loss: 2.292112\n",
      "Epoch 155, loss: 2.289412\n",
      "Epoch 156, loss: 2.287292\n",
      "Epoch 157, loss: 2.290996\n",
      "Epoch 158, loss: 2.284182\n",
      "Epoch 159, loss: 2.291997\n",
      "Epoch 160, loss: 2.284877\n",
      "Epoch 161, loss: 2.286529\n",
      "Epoch 162, loss: 2.286123\n",
      "Epoch 163, loss: 2.286969\n",
      "Epoch 164, loss: 2.287640\n",
      "Epoch 165, loss: 2.289099\n",
      "Epoch 166, loss: 2.289076\n",
      "Epoch 167, loss: 2.288547\n",
      "Epoch 168, loss: 2.288591\n",
      "Epoch 169, loss: 2.284904\n",
      "Epoch 170, loss: 2.282894\n",
      "Epoch 171, loss: 2.290707\n",
      "Epoch 172, loss: 2.292084\n",
      "Epoch 173, loss: 2.283298\n",
      "Epoch 174, loss: 2.289443\n",
      "Epoch 175, loss: 2.285342\n",
      "Epoch 176, loss: 2.287278\n",
      "Epoch 177, loss: 2.287203\n",
      "Epoch 178, loss: 2.289385\n",
      "Epoch 179, loss: 2.283071\n",
      "Epoch 180, loss: 2.284374\n",
      "Epoch 181, loss: 2.288367\n",
      "Epoch 182, loss: 2.283737\n",
      "Epoch 183, loss: 2.287114\n",
      "Epoch 184, loss: 2.281571\n",
      "Epoch 185, loss: 2.289446\n",
      "Epoch 186, loss: 2.285060\n",
      "Epoch 187, loss: 2.285266\n",
      "Epoch 188, loss: 2.286638\n",
      "Epoch 189, loss: 2.282841\n",
      "Epoch 190, loss: 2.286934\n",
      "Epoch 191, loss: 2.287607\n",
      "Epoch 192, loss: 2.287145\n",
      "Epoch 193, loss: 2.289390\n",
      "Epoch 194, loss: 2.281541\n",
      "Epoch 195, loss: 2.288672\n",
      "Epoch 196, loss: 2.283041\n",
      "Epoch 197, loss: 2.290286\n",
      "Epoch 198, loss: 2.282672\n",
      "Epoch 199, loss: 2.286078\n",
      "Epoch 0, loss: 2.303756\n",
      "Epoch 1, loss: 2.302231\n",
      "Epoch 2, loss: 2.302897\n",
      "Epoch 3, loss: 2.302363\n",
      "Epoch 4, loss: 2.302900\n",
      "Epoch 5, loss: 2.303345\n",
      "Epoch 6, loss: 2.303630\n",
      "Epoch 7, loss: 2.303360\n",
      "Epoch 8, loss: 2.302920\n",
      "Epoch 9, loss: 2.303271\n",
      "Epoch 10, loss: 2.303304\n",
      "Epoch 11, loss: 2.303219\n",
      "Epoch 12, loss: 2.301954\n",
      "Epoch 13, loss: 2.302295\n",
      "Epoch 14, loss: 2.303103\n",
      "Epoch 15, loss: 2.302791\n",
      "Epoch 16, loss: 2.302818\n",
      "Epoch 17, loss: 2.302578\n",
      "Epoch 18, loss: 2.302525\n",
      "Epoch 19, loss: 2.302055\n",
      "Epoch 20, loss: 2.302547\n",
      "Epoch 21, loss: 2.301640\n",
      "Epoch 22, loss: 2.302740\n",
      "Epoch 23, loss: 2.302315\n",
      "Epoch 24, loss: 2.302214\n",
      "Epoch 25, loss: 2.302546\n",
      "Epoch 26, loss: 2.302469\n",
      "Epoch 27, loss: 2.303342\n",
      "Epoch 28, loss: 2.303218\n",
      "Epoch 29, loss: 2.302146\n",
      "Epoch 30, loss: 2.302652\n",
      "Epoch 31, loss: 2.302444\n",
      "Epoch 32, loss: 2.302772\n",
      "Epoch 33, loss: 2.302957\n",
      "Epoch 34, loss: 2.301985\n",
      "Epoch 35, loss: 2.302478\n",
      "Epoch 36, loss: 2.302234\n",
      "Epoch 37, loss: 2.302124\n",
      "Epoch 38, loss: 2.302994\n",
      "Epoch 39, loss: 2.302521\n",
      "Epoch 40, loss: 2.301140\n",
      "Epoch 41, loss: 2.301634\n",
      "Epoch 42, loss: 2.301349\n",
      "Epoch 43, loss: 2.302140\n",
      "Epoch 44, loss: 2.302211\n",
      "Epoch 45, loss: 2.301928\n",
      "Epoch 46, loss: 2.302669\n",
      "Epoch 47, loss: 2.302217\n",
      "Epoch 48, loss: 2.302589\n",
      "Epoch 49, loss: 2.302072\n",
      "Epoch 50, loss: 2.303209\n",
      "Epoch 51, loss: 2.302179\n",
      "Epoch 52, loss: 2.302434\n",
      "Epoch 53, loss: 2.302259\n",
      "Epoch 54, loss: 2.301802\n",
      "Epoch 55, loss: 2.302242\n",
      "Epoch 56, loss: 2.301562\n",
      "Epoch 57, loss: 2.301831\n",
      "Epoch 58, loss: 2.302471\n",
      "Epoch 59, loss: 2.302511\n",
      "Epoch 60, loss: 2.302455\n",
      "Epoch 61, loss: 2.302410\n",
      "Epoch 62, loss: 2.301881\n",
      "Epoch 63, loss: 2.303296\n",
      "Epoch 64, loss: 2.301884\n",
      "Epoch 65, loss: 2.302480\n",
      "Epoch 66, loss: 2.302381\n",
      "Epoch 67, loss: 2.302152\n",
      "Epoch 68, loss: 2.301907\n",
      "Epoch 69, loss: 2.301620\n",
      "Epoch 70, loss: 2.302118\n",
      "Epoch 71, loss: 2.302094\n",
      "Epoch 72, loss: 2.301347\n",
      "Epoch 73, loss: 2.302316\n",
      "Epoch 74, loss: 2.301253\n",
      "Epoch 75, loss: 2.302487\n",
      "Epoch 76, loss: 2.301912\n",
      "Epoch 77, loss: 2.301325\n",
      "Epoch 78, loss: 2.301596\n",
      "Epoch 79, loss: 2.301465\n",
      "Epoch 80, loss: 2.302042\n",
      "Epoch 81, loss: 2.301254\n",
      "Epoch 82, loss: 2.301764\n",
      "Epoch 83, loss: 2.300992\n",
      "Epoch 84, loss: 2.301765\n",
      "Epoch 85, loss: 2.301909\n",
      "Epoch 86, loss: 2.302313\n",
      "Epoch 87, loss: 2.301543\n",
      "Epoch 88, loss: 2.302665\n",
      "Epoch 89, loss: 2.301650\n",
      "Epoch 90, loss: 2.301894\n",
      "Epoch 91, loss: 2.302968\n",
      "Epoch 92, loss: 2.301518\n",
      "Epoch 93, loss: 2.300595\n",
      "Epoch 94, loss: 2.301855\n",
      "Epoch 95, loss: 2.301114\n",
      "Epoch 96, loss: 2.301340\n",
      "Epoch 97, loss: 2.302212\n",
      "Epoch 98, loss: 2.302043\n",
      "Epoch 99, loss: 2.301109\n",
      "Epoch 100, loss: 2.302461\n",
      "Epoch 101, loss: 2.301771\n",
      "Epoch 102, loss: 2.301782\n",
      "Epoch 103, loss: 2.301597\n",
      "Epoch 104, loss: 2.300188\n",
      "Epoch 105, loss: 2.302273\n",
      "Epoch 106, loss: 2.300732\n",
      "Epoch 107, loss: 2.300658\n",
      "Epoch 108, loss: 2.302007\n",
      "Epoch 109, loss: 2.301499\n",
      "Epoch 110, loss: 2.301390\n",
      "Epoch 111, loss: 2.300341\n",
      "Epoch 112, loss: 2.302190\n",
      "Epoch 113, loss: 2.302200\n",
      "Epoch 114, loss: 2.302437\n",
      "Epoch 115, loss: 2.300771\n",
      "Epoch 116, loss: 2.301626\n",
      "Epoch 117, loss: 2.301571\n",
      "Epoch 118, loss: 2.302194\n",
      "Epoch 119, loss: 2.301410\n",
      "Epoch 120, loss: 2.301468\n",
      "Epoch 121, loss: 2.300891\n",
      "Epoch 122, loss: 2.301594\n",
      "Epoch 123, loss: 2.301638\n",
      "Epoch 124, loss: 2.301214\n",
      "Epoch 125, loss: 2.300804\n",
      "Epoch 126, loss: 2.300941\n",
      "Epoch 127, loss: 2.301088\n",
      "Epoch 128, loss: 2.301480\n",
      "Epoch 129, loss: 2.301277\n",
      "Epoch 130, loss: 2.301273\n",
      "Epoch 131, loss: 2.300950\n",
      "Epoch 132, loss: 2.301739\n",
      "Epoch 133, loss: 2.301452\n",
      "Epoch 134, loss: 2.301373\n",
      "Epoch 135, loss: 2.302205\n",
      "Epoch 136, loss: 2.301469\n",
      "Epoch 137, loss: 2.301998\n",
      "Epoch 138, loss: 2.301479\n",
      "Epoch 139, loss: 2.300775\n",
      "Epoch 140, loss: 2.300933\n",
      "Epoch 141, loss: 2.301717\n",
      "Epoch 142, loss: 2.302391\n",
      "Epoch 143, loss: 2.300857\n",
      "Epoch 144, loss: 2.301292\n",
      "Epoch 145, loss: 2.300930\n",
      "Epoch 146, loss: 2.300780\n",
      "Epoch 147, loss: 2.300894\n",
      "Epoch 148, loss: 2.301543\n",
      "Epoch 149, loss: 2.301253\n",
      "Epoch 150, loss: 2.301467\n",
      "Epoch 151, loss: 2.301316\n",
      "Epoch 152, loss: 2.301253\n",
      "Epoch 153, loss: 2.300949\n",
      "Epoch 154, loss: 2.301552\n",
      "Epoch 155, loss: 2.300090\n",
      "Epoch 156, loss: 2.300117\n",
      "Epoch 157, loss: 2.301951\n",
      "Epoch 158, loss: 2.300768\n",
      "Epoch 159, loss: 2.300333\n",
      "Epoch 160, loss: 2.300756\n",
      "Epoch 161, loss: 2.301340\n",
      "Epoch 162, loss: 2.302103\n",
      "Epoch 163, loss: 2.300570\n",
      "Epoch 164, loss: 2.300818\n",
      "Epoch 165, loss: 2.300690\n",
      "Epoch 166, loss: 2.301501\n",
      "Epoch 167, loss: 2.301028\n",
      "Epoch 168, loss: 2.300305\n",
      "Epoch 169, loss: 2.300642\n",
      "Epoch 170, loss: 2.300368\n",
      "Epoch 171, loss: 2.299407\n",
      "Epoch 172, loss: 2.301883\n",
      "Epoch 173, loss: 2.301325\n",
      "Epoch 174, loss: 2.301553\n",
      "Epoch 175, loss: 2.301543\n",
      "Epoch 176, loss: 2.301527\n",
      "Epoch 177, loss: 2.300510\n",
      "Epoch 178, loss: 2.300592\n",
      "Epoch 179, loss: 2.301126\n",
      "Epoch 180, loss: 2.301652\n",
      "Epoch 181, loss: 2.300362\n",
      "Epoch 182, loss: 2.299831\n",
      "Epoch 183, loss: 2.300033\n",
      "Epoch 184, loss: 2.299508\n",
      "Epoch 185, loss: 2.299659\n",
      "Epoch 186, loss: 2.301495\n",
      "Epoch 187, loss: 2.301892\n",
      "Epoch 188, loss: 2.300460\n",
      "Epoch 189, loss: 2.300816\n",
      "Epoch 190, loss: 2.300293\n",
      "Epoch 191, loss: 2.300057\n",
      "Epoch 192, loss: 2.300816\n",
      "Epoch 193, loss: 2.301244\n",
      "Epoch 194, loss: 2.301085\n",
      "Epoch 195, loss: 2.300595\n",
      "Epoch 196, loss: 2.299911\n",
      "Epoch 197, loss: 2.299409\n",
      "Epoch 198, loss: 2.300056\n",
      "Epoch 199, loss: 2.301386\n",
      "Epoch 0, loss: 2.302136\n",
      "Epoch 1, loss: 2.302063\n",
      "Epoch 2, loss: 2.302097\n",
      "Epoch 3, loss: 2.302193\n",
      "Epoch 4, loss: 2.301875\n",
      "Epoch 5, loss: 2.302004\n",
      "Epoch 6, loss: 2.303297\n",
      "Epoch 7, loss: 2.303182\n",
      "Epoch 8, loss: 2.302106\n",
      "Epoch 9, loss: 2.302425\n",
      "Epoch 10, loss: 2.302929\n",
      "Epoch 11, loss: 2.304059\n",
      "Epoch 12, loss: 2.303546\n",
      "Epoch 13, loss: 2.302980\n",
      "Epoch 14, loss: 2.302754\n",
      "Epoch 15, loss: 2.302764\n",
      "Epoch 16, loss: 2.301993\n",
      "Epoch 17, loss: 2.303138\n",
      "Epoch 18, loss: 2.302323\n",
      "Epoch 19, loss: 2.302066\n",
      "Epoch 20, loss: 2.303106\n",
      "Epoch 21, loss: 2.302148\n",
      "Epoch 22, loss: 2.301314\n",
      "Epoch 23, loss: 2.302103\n",
      "Epoch 24, loss: 2.302190\n",
      "Epoch 25, loss: 2.303771\n",
      "Epoch 26, loss: 2.302701\n",
      "Epoch 27, loss: 2.301833\n",
      "Epoch 28, loss: 2.303011\n",
      "Epoch 29, loss: 2.301961\n",
      "Epoch 30, loss: 2.302674\n",
      "Epoch 31, loss: 2.301912\n",
      "Epoch 32, loss: 2.301388\n",
      "Epoch 33, loss: 2.301721\n",
      "Epoch 34, loss: 2.301066\n",
      "Epoch 35, loss: 2.302158\n",
      "Epoch 36, loss: 2.301535\n",
      "Epoch 37, loss: 2.301532\n",
      "Epoch 38, loss: 2.301996\n",
      "Epoch 39, loss: 2.302273\n",
      "Epoch 40, loss: 2.302048\n",
      "Epoch 41, loss: 2.301700\n",
      "Epoch 42, loss: 2.302293\n",
      "Epoch 43, loss: 2.301862\n",
      "Epoch 44, loss: 2.302210\n",
      "Epoch 45, loss: 2.302477\n",
      "Epoch 46, loss: 2.302039\n",
      "Epoch 47, loss: 2.302930\n",
      "Epoch 48, loss: 2.303152\n",
      "Epoch 49, loss: 2.301706\n",
      "Epoch 50, loss: 2.301553\n",
      "Epoch 51, loss: 2.301089\n",
      "Epoch 52, loss: 2.303705\n",
      "Epoch 53, loss: 2.302265\n",
      "Epoch 54, loss: 2.301793\n",
      "Epoch 55, loss: 2.302035\n",
      "Epoch 56, loss: 2.302886\n",
      "Epoch 57, loss: 2.301271\n",
      "Epoch 58, loss: 2.301373\n",
      "Epoch 59, loss: 2.301914\n",
      "Epoch 60, loss: 2.301457\n",
      "Epoch 61, loss: 2.301291\n",
      "Epoch 62, loss: 2.301164\n",
      "Epoch 63, loss: 2.301899\n",
      "Epoch 64, loss: 2.301448\n",
      "Epoch 65, loss: 2.302024\n",
      "Epoch 66, loss: 2.302425\n",
      "Epoch 67, loss: 2.302260\n",
      "Epoch 68, loss: 2.301266\n",
      "Epoch 69, loss: 2.301696\n",
      "Epoch 70, loss: 2.300638\n",
      "Epoch 71, loss: 2.302260\n",
      "Epoch 72, loss: 2.302723\n",
      "Epoch 73, loss: 2.301178\n",
      "Epoch 74, loss: 2.300812\n",
      "Epoch 75, loss: 2.302220\n",
      "Epoch 76, loss: 2.301738\n",
      "Epoch 77, loss: 2.301053\n",
      "Epoch 78, loss: 2.302161\n",
      "Epoch 79, loss: 2.300865\n",
      "Epoch 80, loss: 2.301367\n",
      "Epoch 81, loss: 2.302369\n",
      "Epoch 82, loss: 2.303273\n",
      "Epoch 83, loss: 2.301722\n",
      "Epoch 84, loss: 2.302075\n",
      "Epoch 85, loss: 2.301507\n",
      "Epoch 86, loss: 2.301536\n",
      "Epoch 87, loss: 2.301191\n",
      "Epoch 88, loss: 2.300779\n",
      "Epoch 89, loss: 2.303102\n",
      "Epoch 90, loss: 2.302059\n",
      "Epoch 91, loss: 2.301541\n",
      "Epoch 92, loss: 2.301607\n",
      "Epoch 93, loss: 2.300593\n",
      "Epoch 94, loss: 2.302255\n",
      "Epoch 95, loss: 2.301811\n",
      "Epoch 96, loss: 2.301503\n",
      "Epoch 97, loss: 2.301880\n",
      "Epoch 98, loss: 2.300369\n",
      "Epoch 99, loss: 2.301925\n",
      "Epoch 100, loss: 2.302353\n",
      "Epoch 101, loss: 2.300920\n",
      "Epoch 102, loss: 2.301420\n",
      "Epoch 103, loss: 2.301892\n",
      "Epoch 104, loss: 2.302077\n",
      "Epoch 105, loss: 2.301090\n",
      "Epoch 106, loss: 2.301676\n",
      "Epoch 107, loss: 2.300871\n",
      "Epoch 108, loss: 2.302280\n",
      "Epoch 109, loss: 2.302318\n",
      "Epoch 110, loss: 2.301349\n",
      "Epoch 111, loss: 2.301389\n",
      "Epoch 112, loss: 2.301878\n",
      "Epoch 113, loss: 2.301568\n",
      "Epoch 114, loss: 2.300154\n",
      "Epoch 115, loss: 2.301015\n",
      "Epoch 116, loss: 2.300820\n",
      "Epoch 117, loss: 2.300883\n",
      "Epoch 118, loss: 2.302166\n",
      "Epoch 119, loss: 2.302264\n",
      "Epoch 120, loss: 2.301587\n",
      "Epoch 121, loss: 2.301760\n",
      "Epoch 122, loss: 2.302019\n",
      "Epoch 123, loss: 2.300951\n",
      "Epoch 124, loss: 2.300905\n",
      "Epoch 125, loss: 2.301553\n",
      "Epoch 126, loss: 2.299846\n",
      "Epoch 127, loss: 2.300478\n",
      "Epoch 128, loss: 2.302392\n",
      "Epoch 129, loss: 2.299215\n",
      "Epoch 130, loss: 2.301521\n",
      "Epoch 131, loss: 2.301744\n",
      "Epoch 132, loss: 2.301198\n",
      "Epoch 133, loss: 2.301147\n",
      "Epoch 134, loss: 2.300695\n",
      "Epoch 135, loss: 2.302028\n",
      "Epoch 136, loss: 2.301339\n",
      "Epoch 137, loss: 2.301076\n",
      "Epoch 138, loss: 2.300834\n",
      "Epoch 139, loss: 2.300400\n",
      "Epoch 140, loss: 2.300644\n",
      "Epoch 141, loss: 2.299533\n",
      "Epoch 142, loss: 2.301485\n",
      "Epoch 143, loss: 2.300110\n",
      "Epoch 144, loss: 2.301668\n",
      "Epoch 145, loss: 2.301592\n",
      "Epoch 146, loss: 2.301361\n",
      "Epoch 147, loss: 2.301453\n",
      "Epoch 148, loss: 2.301149\n",
      "Epoch 149, loss: 2.301522\n",
      "Epoch 150, loss: 2.301012\n",
      "Epoch 151, loss: 2.300794\n",
      "Epoch 152, loss: 2.301072\n",
      "Epoch 153, loss: 2.300552\n",
      "Epoch 154, loss: 2.301955\n",
      "Epoch 155, loss: 2.301708\n",
      "Epoch 156, loss: 2.300032\n",
      "Epoch 157, loss: 2.300087\n",
      "Epoch 158, loss: 2.300235\n",
      "Epoch 159, loss: 2.301946\n",
      "Epoch 160, loss: 2.300753\n",
      "Epoch 161, loss: 2.300619\n",
      "Epoch 162, loss: 2.300581\n",
      "Epoch 163, loss: 2.301195\n",
      "Epoch 164, loss: 2.298567\n",
      "Epoch 165, loss: 2.302769\n",
      "Epoch 166, loss: 2.301068\n",
      "Epoch 167, loss: 2.300198\n",
      "Epoch 168, loss: 2.301429\n",
      "Epoch 169, loss: 2.300128\n",
      "Epoch 170, loss: 2.298825\n",
      "Epoch 171, loss: 2.301406\n",
      "Epoch 172, loss: 2.301772\n",
      "Epoch 173, loss: 2.302464\n",
      "Epoch 174, loss: 2.300687\n",
      "Epoch 175, loss: 2.301035\n",
      "Epoch 176, loss: 2.300077\n",
      "Epoch 177, loss: 2.301863\n",
      "Epoch 178, loss: 2.299909\n",
      "Epoch 179, loss: 2.299802\n",
      "Epoch 180, loss: 2.300463\n",
      "Epoch 181, loss: 2.299943\n",
      "Epoch 182, loss: 2.300718\n",
      "Epoch 183, loss: 2.299823\n",
      "Epoch 184, loss: 2.299228\n",
      "Epoch 185, loss: 2.300182\n",
      "Epoch 186, loss: 2.300156\n",
      "Epoch 187, loss: 2.299495\n",
      "Epoch 188, loss: 2.299825\n",
      "Epoch 189, loss: 2.300231\n",
      "Epoch 190, loss: 2.300475\n",
      "Epoch 191, loss: 2.300023\n",
      "Epoch 192, loss: 2.301066\n",
      "Epoch 193, loss: 2.299202\n",
      "Epoch 194, loss: 2.300747\n",
      "Epoch 195, loss: 2.300667\n",
      "Epoch 196, loss: 2.301707\n",
      "Epoch 197, loss: 2.299782\n",
      "Epoch 198, loss: 2.300290\n",
      "Epoch 199, loss: 2.301882\n",
      "Epoch 0, loss: 2.302488\n",
      "Epoch 1, loss: 2.302668\n",
      "Epoch 2, loss: 2.301558\n",
      "Epoch 3, loss: 2.302353\n",
      "Epoch 4, loss: 2.303242\n",
      "Epoch 5, loss: 2.302640\n",
      "Epoch 6, loss: 2.302925\n",
      "Epoch 7, loss: 2.303245\n",
      "Epoch 8, loss: 2.302860\n",
      "Epoch 9, loss: 2.301845\n",
      "Epoch 10, loss: 2.302271\n",
      "Epoch 11, loss: 2.302665\n",
      "Epoch 12, loss: 2.303139\n",
      "Epoch 13, loss: 2.302695\n",
      "Epoch 14, loss: 2.303667\n",
      "Epoch 15, loss: 2.303463\n",
      "Epoch 16, loss: 2.302286\n",
      "Epoch 17, loss: 2.301834\n",
      "Epoch 18, loss: 2.302311\n",
      "Epoch 19, loss: 2.302096\n",
      "Epoch 20, loss: 2.302144\n",
      "Epoch 21, loss: 2.301826\n",
      "Epoch 22, loss: 2.303153\n",
      "Epoch 23, loss: 2.302193\n",
      "Epoch 24, loss: 2.302617\n",
      "Epoch 25, loss: 2.302409\n",
      "Epoch 26, loss: 2.303168\n",
      "Epoch 27, loss: 2.302254\n",
      "Epoch 28, loss: 2.302399\n",
      "Epoch 29, loss: 2.303790\n",
      "Epoch 30, loss: 2.302436\n",
      "Epoch 31, loss: 2.301773\n",
      "Epoch 32, loss: 2.303545\n",
      "Epoch 33, loss: 2.302176\n",
      "Epoch 34, loss: 2.303002\n",
      "Epoch 35, loss: 2.302249\n",
      "Epoch 36, loss: 2.301868\n",
      "Epoch 37, loss: 2.301980\n",
      "Epoch 38, loss: 2.303235\n",
      "Epoch 39, loss: 2.301966\n",
      "Epoch 40, loss: 2.301662\n",
      "Epoch 41, loss: 2.302431\n",
      "Epoch 42, loss: 2.302754\n",
      "Epoch 43, loss: 2.302911\n",
      "Epoch 44, loss: 2.301210\n",
      "Epoch 45, loss: 2.302760\n",
      "Epoch 46, loss: 2.303102\n",
      "Epoch 47, loss: 2.301627\n",
      "Epoch 48, loss: 2.303002\n",
      "Epoch 49, loss: 2.302480\n",
      "Epoch 50, loss: 2.302283\n",
      "Epoch 51, loss: 2.302817\n",
      "Epoch 52, loss: 2.302105\n",
      "Epoch 53, loss: 2.301548\n",
      "Epoch 54, loss: 2.302336\n",
      "Epoch 55, loss: 2.302768\n",
      "Epoch 56, loss: 2.301633\n",
      "Epoch 57, loss: 2.302026\n",
      "Epoch 58, loss: 2.302554\n",
      "Epoch 59, loss: 2.303449\n",
      "Epoch 60, loss: 2.302689\n",
      "Epoch 61, loss: 2.302346\n",
      "Epoch 62, loss: 2.303643\n",
      "Epoch 63, loss: 2.302438\n",
      "Epoch 64, loss: 2.303382\n",
      "Epoch 65, loss: 2.302468\n",
      "Epoch 66, loss: 2.302466\n",
      "Epoch 67, loss: 2.302270\n",
      "Epoch 68, loss: 2.301629\n",
      "Epoch 69, loss: 2.300944\n",
      "Epoch 70, loss: 2.301551\n",
      "Epoch 71, loss: 2.302132\n",
      "Epoch 72, loss: 2.302594\n",
      "Epoch 73, loss: 2.302023\n",
      "Epoch 74, loss: 2.301257\n",
      "Epoch 75, loss: 2.302819\n",
      "Epoch 76, loss: 2.301148\n",
      "Epoch 77, loss: 2.301773\n",
      "Epoch 78, loss: 2.302345\n",
      "Epoch 79, loss: 2.301132\n",
      "Epoch 80, loss: 2.302709\n",
      "Epoch 81, loss: 2.301916\n",
      "Epoch 82, loss: 2.301018\n",
      "Epoch 83, loss: 2.301386\n",
      "Epoch 84, loss: 2.302325\n",
      "Epoch 85, loss: 2.300919\n",
      "Epoch 86, loss: 2.301148\n",
      "Epoch 87, loss: 2.300603\n",
      "Epoch 88, loss: 2.301195\n",
      "Epoch 89, loss: 2.301512\n",
      "Epoch 90, loss: 2.301979\n",
      "Epoch 91, loss: 2.302531\n",
      "Epoch 92, loss: 2.301806\n",
      "Epoch 93, loss: 2.300899\n",
      "Epoch 94, loss: 2.302049\n",
      "Epoch 95, loss: 2.301929\n",
      "Epoch 96, loss: 2.301458\n",
      "Epoch 97, loss: 2.301681\n",
      "Epoch 98, loss: 2.301026\n",
      "Epoch 99, loss: 2.302995\n",
      "Epoch 100, loss: 2.301351\n",
      "Epoch 101, loss: 2.303162\n",
      "Epoch 102, loss: 2.301272\n",
      "Epoch 103, loss: 2.302115\n",
      "Epoch 104, loss: 2.300868\n",
      "Epoch 105, loss: 2.301456\n",
      "Epoch 106, loss: 2.302159\n",
      "Epoch 107, loss: 2.302704\n",
      "Epoch 108, loss: 2.301624\n",
      "Epoch 109, loss: 2.301623\n",
      "Epoch 110, loss: 2.301934\n",
      "Epoch 111, loss: 2.300734\n",
      "Epoch 112, loss: 2.302139\n",
      "Epoch 113, loss: 2.301621\n",
      "Epoch 114, loss: 2.302567\n",
      "Epoch 115, loss: 2.302031\n",
      "Epoch 116, loss: 2.300727\n",
      "Epoch 117, loss: 2.302123\n",
      "Epoch 118, loss: 2.302091\n",
      "Epoch 119, loss: 2.301351\n",
      "Epoch 120, loss: 2.300689\n",
      "Epoch 121, loss: 2.301636\n",
      "Epoch 122, loss: 2.301454\n",
      "Epoch 123, loss: 2.300708\n",
      "Epoch 124, loss: 2.300442\n",
      "Epoch 125, loss: 2.301565\n",
      "Epoch 126, loss: 2.300231\n",
      "Epoch 127, loss: 2.301414\n",
      "Epoch 128, loss: 2.300669\n",
      "Epoch 129, loss: 2.301532\n",
      "Epoch 130, loss: 2.301288\n",
      "Epoch 131, loss: 2.301666\n",
      "Epoch 132, loss: 2.301555\n",
      "Epoch 133, loss: 2.300463\n",
      "Epoch 134, loss: 2.301926\n",
      "Epoch 135, loss: 2.301443\n",
      "Epoch 136, loss: 2.302068\n",
      "Epoch 137, loss: 2.300570\n",
      "Epoch 138, loss: 2.301013\n",
      "Epoch 139, loss: 2.300749\n",
      "Epoch 140, loss: 2.301389\n",
      "Epoch 141, loss: 2.301911\n",
      "Epoch 142, loss: 2.301437\n",
      "Epoch 143, loss: 2.301316\n",
      "Epoch 144, loss: 2.301451\n",
      "Epoch 145, loss: 2.301771\n",
      "Epoch 146, loss: 2.301169\n",
      "Epoch 147, loss: 2.301846\n",
      "Epoch 148, loss: 2.301238\n",
      "Epoch 149, loss: 2.302289\n",
      "Epoch 150, loss: 2.300232\n",
      "Epoch 151, loss: 2.300817\n",
      "Epoch 152, loss: 2.301271\n",
      "Epoch 153, loss: 2.301517\n",
      "Epoch 154, loss: 2.302699\n",
      "Epoch 155, loss: 2.300962\n",
      "Epoch 156, loss: 2.300561\n",
      "Epoch 157, loss: 2.301002\n",
      "Epoch 158, loss: 2.301781\n",
      "Epoch 159, loss: 2.300571\n",
      "Epoch 160, loss: 2.300718\n",
      "Epoch 161, loss: 2.300655\n",
      "Epoch 162, loss: 2.301301\n",
      "Epoch 163, loss: 2.300319\n",
      "Epoch 164, loss: 2.301772\n",
      "Epoch 165, loss: 2.299693\n",
      "Epoch 166, loss: 2.300259\n",
      "Epoch 167, loss: 2.299903\n",
      "Epoch 168, loss: 2.302217\n",
      "Epoch 169, loss: 2.300636\n",
      "Epoch 170, loss: 2.300673\n",
      "Epoch 171, loss: 2.300635\n",
      "Epoch 172, loss: 2.301892\n",
      "Epoch 173, loss: 2.300153\n",
      "Epoch 174, loss: 2.300285\n",
      "Epoch 175, loss: 2.301335\n",
      "Epoch 176, loss: 2.300161\n",
      "Epoch 177, loss: 2.302082\n",
      "Epoch 178, loss: 2.299539\n",
      "Epoch 179, loss: 2.300999\n",
      "Epoch 180, loss: 2.299778\n",
      "Epoch 181, loss: 2.301107\n",
      "Epoch 182, loss: 2.301724\n",
      "Epoch 183, loss: 2.300619\n",
      "Epoch 184, loss: 2.301465\n",
      "Epoch 185, loss: 2.302565\n",
      "Epoch 186, loss: 2.299868\n",
      "Epoch 187, loss: 2.301396\n",
      "Epoch 188, loss: 2.300572\n",
      "Epoch 189, loss: 2.301121\n",
      "Epoch 190, loss: 2.301882\n",
      "Epoch 191, loss: 2.301751\n",
      "Epoch 192, loss: 2.300257\n",
      "Epoch 193, loss: 2.301492\n",
      "Epoch 194, loss: 2.300729\n",
      "Epoch 195, loss: 2.301139\n",
      "Epoch 196, loss: 2.300323\n",
      "Epoch 197, loss: 2.301402\n",
      "Epoch 198, loss: 2.300555\n",
      "Epoch 199, loss: 2.299162\n",
      "best validation accuracy achieved: 0.228000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs,\n",
    "                                      learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "            best_lr, best_rs = lr, rs\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.196000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
