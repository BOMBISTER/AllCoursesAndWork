{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XDg7nyHsgKs"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P59NYU98GCb9",
        "outputId": "652a8b96-c39c-4b01-917e-6eb5913262aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==0.4.1 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==0.4.1\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (scikit-learn)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8sVtGHmA9aBM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiA2dGmgF1rW",
        "outputId": "4fccf8ac-947e-4e2d-be78-17f0d0a6bb74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QstS4NO0L97c",
        "outputId": "defb6340-cdea-4b0b-e6b3-a85a96ad730b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ]
        }
      ],
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTai8Ta0lgwL",
        "outputId": "33126f07-540f-48ce-f4b6-fb5c66fe49d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCjwwDs6Zq9x",
        "outputId": "03f78684-7c1f-4ef5-c48c-7372a91c42fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in train = 45441. Tags = {'VERB', 'PRT', 'NUM', 'CONJ', 'PRON', 'X', 'NOUN', 'ADV', '.', 'DET', 'ADP', 'ADJ'}\n"
          ]
        }
      ],
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "URC1B2nvPGFt",
        "outputId": "d0046270-c835-49cc-f29d-4836f46724eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rWmSToIaeAo",
        "outputId": "a50cf416-b783-4def-ba26-1dc1bf3347cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-747c80ba0bc0>:6: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjz_Rk0bbMyH",
        "outputId": "5cc7aaa6-c2fc-4431-91b4-8dd5f5cab83d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-0f47def21c45>:2: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ]
        }
      ],
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XCuxEBVbOY_",
        "outputId": "2ad7b03b-be8f-49d1-97cd-fa4e6f09a317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-89f0185880b7>:2: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ]
        }
      ],
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RtRbz1SwgEqc"
      },
      "outputs": [],
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DhsTKZalfih6"
      },
      "outputs": [],
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "\n",
        "        batch_indices = indices[start:end]\n",
        "\n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "\n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "\n",
        "        yield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4XsRII5kW5x",
        "outputId": "3a86abfc-1b94-45bf-b241-9628ef943383"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WVEHju54d68T"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(input_size = word_emb_dim,\n",
        "                            hidden_size = lstm_hidden_dim,\n",
        "                            num_layers = lstm_layers_count,\n",
        "                            bias = True)\n",
        "\n",
        "        self.linear = nn.Linear(lstm_hidden_dim, tagset_size, bias=False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_out = self.embedding(inputs)\n",
        "\n",
        "        lstm_out, _ = self.lstm.forward(embedding_out)\n",
        "        linear_out = self.linear.forward(lstm_out)\n",
        "\n",
        "        return linear_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbrxsZ2mehWB",
        "outputId": "0fb917c5-29f9-4d1d-d434-f23883f28fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.065\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "indices_log = torch.max(logits, dim=2)[1]\n",
        "\n",
        "correct_samples = float(torch.sum(indices_log[y_batch!=0] == y_batch[y_batch!=0]))\n",
        "print('accuracy = {:.3f}'.format(correct_samples / (y_batch[y_batch!=0].shape[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMUyUm1hgpe3",
        "outputId": "476b1d01-dd28-46e7-9260-4aca274ee8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.6280\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "test = logits.type(torch.FloatTensor)\n",
        "loss = criterion(logits.view(-1,logits.shape[-1]), y_batch.view(-1))\n",
        "\n",
        "print(f'Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FprPQ0gllo7b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "\n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "\n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "\n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                indices_log = torch.max(logits, dim=2)[1]\n",
        "                loss = criterion(logits.view(-1,logits.shape[-1]), y_batch.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                correct_samples = float(torch.sum(indices_log[y_batch!=0] == y_batch[y_batch!=0]))\n",
        "                cur_correct_count, cur_sum_count =  correct_samples, y_batch[y_batch!=0].shape[0]\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "\n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "\n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "\n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqfbeh1ltEYa",
        "outputId": "0f3e6b58-aac9-4e3d-9170-3813cfde57ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/572 [00:00<?, ?it/s]<ipython-input-22-5f495c93e9e9>:19: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
            "[1 / 20] Train: Loss = 0.31108, Accuracy = 72.26%: 100%|██████████| 572/572 [00:04<00:00, 114.56it/s]\n",
            "[1 / 20]   Val: Loss = 0.10210, Accuracy = 85.34%: 100%|██████████| 13/13 [00:00<00:00, 83.57it/s]\n",
            "[2 / 20] Train: Loss = 0.10058, Accuracy = 89.96%: 100%|██████████| 572/572 [00:03<00:00, 163.30it/s]\n",
            "[2 / 20]   Val: Loss = 0.07244, Accuracy = 89.50%: 100%|██████████| 13/13 [00:00<00:00, 81.81it/s]\n",
            "[3 / 20] Train: Loss = 0.06760, Accuracy = 93.22%: 100%|██████████| 572/572 [00:03<00:00, 162.79it/s]\n",
            "[3 / 20]   Val: Loss = 0.06380, Accuracy = 91.33%: 100%|██████████| 13/13 [00:00<00:00, 82.05it/s]\n",
            "[4 / 20] Train: Loss = 0.05045, Accuracy = 94.85%: 100%|██████████| 572/572 [00:03<00:00, 154.06it/s]\n",
            "[4 / 20]   Val: Loss = 0.06310, Accuracy = 92.16%: 100%|██████████| 13/13 [00:00<00:00, 74.99it/s]\n",
            "[5 / 20] Train: Loss = 0.04046, Accuracy = 95.87%: 100%|██████████| 572/572 [00:03<00:00, 158.33it/s]\n",
            "[5 / 20]   Val: Loss = 0.06195, Accuracy = 92.67%: 100%|██████████| 13/13 [00:00<00:00, 82.25it/s]\n",
            "[6 / 20] Train: Loss = 0.03284, Accuracy = 96.60%: 100%|██████████| 572/572 [00:03<00:00, 163.88it/s]\n",
            "[6 / 20]   Val: Loss = 0.06108, Accuracy = 92.92%: 100%|██████████| 13/13 [00:00<00:00, 80.58it/s]\n",
            "[7 / 20] Train: Loss = 0.02729, Accuracy = 97.17%: 100%|██████████| 572/572 [00:03<00:00, 160.33it/s]\n",
            "[7 / 20]   Val: Loss = 0.06244, Accuracy = 93.02%: 100%|██████████| 13/13 [00:00<00:00, 73.90it/s]\n",
            "[8 / 20] Train: Loss = 0.02252, Accuracy = 97.64%: 100%|██████████| 572/572 [00:03<00:00, 149.32it/s]\n",
            "[8 / 20]   Val: Loss = 0.06396, Accuracy = 93.14%: 100%|██████████| 13/13 [00:00<00:00, 80.50it/s]\n",
            "[9 / 20] Train: Loss = 0.01868, Accuracy = 98.04%: 100%|██████████| 572/572 [00:03<00:00, 162.70it/s]\n",
            "[9 / 20]   Val: Loss = 0.07090, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 83.51it/s]\n",
            "[10 / 20] Train: Loss = 0.01561, Accuracy = 98.39%: 100%|██████████| 572/572 [00:03<00:00, 163.90it/s]\n",
            "[10 / 20]   Val: Loss = 0.07319, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 84.27it/s]\n",
            "[11 / 20] Train: Loss = 0.01292, Accuracy = 98.66%: 100%|██████████| 572/572 [00:03<00:00, 146.33it/s]\n",
            "[11 / 20]   Val: Loss = 0.06795, Accuracy = 93.21%: 100%|██████████| 13/13 [00:00<00:00, 77.74it/s]\n",
            "[12 / 20] Train: Loss = 0.01066, Accuracy = 98.93%: 100%|██████████| 572/572 [00:03<00:00, 161.52it/s]\n",
            "[12 / 20]   Val: Loss = 0.07649, Accuracy = 93.24%: 100%|██████████| 13/13 [00:00<00:00, 81.51it/s]\n",
            "[13 / 20] Train: Loss = 0.00866, Accuracy = 99.13%: 100%|██████████| 572/572 [00:03<00:00, 162.41it/s]\n",
            "[13 / 20]   Val: Loss = 0.07416, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 77.69it/s]\n",
            "[14 / 20] Train: Loss = 0.00714, Accuracy = 99.30%: 100%|██████████| 572/572 [00:03<00:00, 150.14it/s]\n",
            "[14 / 20]   Val: Loss = 0.07654, Accuracy = 93.14%: 100%|██████████| 13/13 [00:00<00:00, 71.01it/s]\n",
            "[15 / 20] Train: Loss = 0.00575, Accuracy = 99.45%: 100%|██████████| 572/572 [00:03<00:00, 162.34it/s]\n",
            "[15 / 20]   Val: Loss = 0.08611, Accuracy = 93.17%: 100%|██████████| 13/13 [00:00<00:00, 80.14it/s]\n",
            "[16 / 20] Train: Loss = 0.00460, Accuracy = 99.57%: 100%|██████████| 572/572 [00:03<00:00, 162.82it/s]\n",
            "[16 / 20]   Val: Loss = 0.08619, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 77.73it/s]\n",
            "[17 / 20] Train: Loss = 0.00379, Accuracy = 99.66%: 100%|██████████| 572/572 [00:03<00:00, 156.50it/s]\n",
            "[17 / 20]   Val: Loss = 0.09179, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 70.23it/s]\n",
            "[18 / 20] Train: Loss = 0.00317, Accuracy = 99.71%: 100%|██████████| 572/572 [00:03<00:00, 153.21it/s]\n",
            "[18 / 20]   Val: Loss = 0.10281, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 79.66it/s]\n",
            "[19 / 20] Train: Loss = 0.00279, Accuracy = 99.75%: 100%|██████████| 572/572 [00:03<00:00, 161.57it/s]\n",
            "[19 / 20]   Val: Loss = 0.10170, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 79.45it/s]\n",
            "[20 / 20] Train: Loss = 0.00247, Accuracy = 99.77%: 100%|██████████| 572/572 [00:03<00:00, 162.45it/s]\n",
            "[20 / 20]   Val: Loss = 0.10843, Accuracy = 92.93%: 100%|██████████| 13/13 [00:00<00:00, 76.03it/s]\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98wr38_rw55D",
        "outputId": "3b71cb31-83ad-4b5d-9c91-c39cbe55235e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 15] Train: Loss = 1.04709, Accuracy = 68.55%: 100%|██████████| 224/224 [00:01<00:00, 145.99it/s]\n",
            "[1 / 15]   Val: Loss = 0.57620, Accuracy = 80.99%: 100%|██████████| 13/13 [00:00<00:00, 71.19it/s]\n",
            "[2 / 15] Train: Loss = 0.46138, Accuracy = 84.71%: 100%|██████████| 224/224 [00:01<00:00, 153.61it/s]\n",
            "[2 / 15]   Val: Loss = 0.39729, Accuracy = 86.79%: 100%|██████████| 13/13 [00:00<00:00, 84.01it/s]\n",
            "[3 / 15] Train: Loss = 0.32928, Accuracy = 89.09%: 100%|██████████| 224/224 [00:01<00:00, 165.51it/s]\n",
            "[3 / 15]   Val: Loss = 0.31587, Accuracy = 89.43%: 100%|██████████| 13/13 [00:00<00:00, 77.98it/s]\n",
            "[4 / 15] Train: Loss = 0.25391, Accuracy = 91.63%: 100%|██████████| 224/224 [00:01<00:00, 163.74it/s]\n",
            "[4 / 15]   Val: Loss = 0.27247, Accuracy = 90.86%: 100%|██████████| 13/13 [00:00<00:00, 81.06it/s]\n",
            "[5 / 15] Train: Loss = 0.20486, Accuracy = 93.30%: 100%|██████████| 224/224 [00:01<00:00, 162.62it/s]\n",
            "[5 / 15]   Val: Loss = 0.24631, Accuracy = 91.78%: 100%|██████████| 13/13 [00:00<00:00, 83.73it/s]\n",
            "[6 / 15] Train: Loss = 0.17007, Accuracy = 94.45%: 100%|██████████| 224/224 [00:01<00:00, 163.78it/s]\n",
            "[6 / 15]   Val: Loss = 0.22927, Accuracy = 92.34%: 100%|██████████| 13/13 [00:00<00:00, 83.49it/s]\n",
            "[7 / 15] Train: Loss = 0.14384, Accuracy = 95.33%: 100%|██████████| 224/224 [00:01<00:00, 163.48it/s]\n",
            "[7 / 15]   Val: Loss = 0.22018, Accuracy = 92.81%: 100%|██████████| 13/13 [00:00<00:00, 84.32it/s]\n",
            "[8 / 15] Train: Loss = 0.12266, Accuracy = 96.01%: 100%|██████████| 224/224 [00:01<00:00, 160.63it/s]\n",
            "[8 / 15]   Val: Loss = 0.21588, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 72.11it/s]\n",
            "[9 / 15] Train: Loss = 0.10589, Accuracy = 96.57%: 100%|██████████| 224/224 [00:01<00:00, 154.86it/s]\n",
            "[9 / 15]   Val: Loss = 0.21589, Accuracy = 93.14%: 100%|██████████| 13/13 [00:00<00:00, 69.11it/s]\n",
            "[10 / 15] Train: Loss = 0.09218, Accuracy = 97.06%: 100%|██████████| 224/224 [00:01<00:00, 148.70it/s]\n",
            "[10 / 15]   Val: Loss = 0.21929, Accuracy = 93.19%: 100%|██████████| 13/13 [00:00<00:00, 78.84it/s]\n",
            "[11 / 15] Train: Loss = 0.08016, Accuracy = 97.44%: 100%|██████████| 224/224 [00:01<00:00, 159.99it/s]\n",
            "[11 / 15]   Val: Loss = 0.22223, Accuracy = 93.31%: 100%|██████████| 13/13 [00:00<00:00, 80.18it/s]\n",
            "[12 / 15] Train: Loss = 0.07026, Accuracy = 97.78%: 100%|██████████| 224/224 [00:01<00:00, 163.37it/s]\n",
            "[12 / 15]   Val: Loss = 0.22672, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 81.06it/s]\n",
            "[13 / 15] Train: Loss = 0.06115, Accuracy = 98.11%: 100%|██████████| 224/224 [00:01<00:00, 166.40it/s]\n",
            "[13 / 15]   Val: Loss = 0.23428, Accuracy = 93.33%: 100%|██████████| 13/13 [00:00<00:00, 77.63it/s]\n",
            "[14 / 15] Train: Loss = 0.05294, Accuracy = 98.37%: 100%|██████████| 224/224 [00:01<00:00, 159.12it/s]\n",
            "[14 / 15]   Val: Loss = 0.24236, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 77.96it/s]\n",
            "[15 / 15] Train: Loss = 0.04593, Accuracy = 98.64%: 100%|██████████| 224/224 [00:01<00:00, 162.07it/s]\n",
            "[15 / 15]   Val: Loss = 0.25252, Accuracy = 93.22%: 100%|██████████| 13/13 [00:00<00:00, 80.95it/s]\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_test, y_test), epochs_count=15,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "test_data=(X_test, y_test)\n",
        "test_loss, test_acc = do_epoch(model, criterion, test_data, batch_size, None, 'Test:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOoB_7egw9BE",
        "outputId": "6f8cbd28-4329-4233-951a-ad1bf6eba6ea"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: Loss = 0.03789, Accuracy = 99.00%: 100%|██████████| 224/224 [00:00<00:00, 325.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_iIt918xsgK1"
      },
      "outputs": [],
      "source": [
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(input_size = word_emb_dim,\n",
        "                            hidden_size = lstm_hidden_dim,\n",
        "                            num_layers = lstm_layers_count,\n",
        "                            bias = True,\n",
        "                            bidirectional = True)\n",
        "\n",
        "        self.linear = nn.Linear(lstm_hidden_dim, tagset_size, bias=False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        embedding_out = self.embedding(inputs)\n",
        "\n",
        "        lstm_out, _ = self.lstm.forward(embedding_out)\n",
        "        linear_out = self.linear.forward(lstm_out)\n",
        "\n",
        "        return linear_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZpY_Q1xZ18h",
        "outputId": "1e69fc9a-129a-4af6-b9c3-321f8037ec22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f85cf6-9871-4a5a-de87-b8e2de16c68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ]
        }
      ],
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "\n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LxaRBpQd0pat"
      },
      "outputs": [],
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding.from_pretrained(embeddings=FloatTensor(embeddings))\n",
        "        self.lstm = nn.LSTM(input_size=embeddings.shape[1],\n",
        "                           hidden_size=lstm_hidden_dim,\n",
        "                           num_layers=lstm_layers_count,\n",
        "                           bidirectional=True)\n",
        "        self.linear = nn.Linear(lstm_hidden_dim*2, tagset_size)\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        embed_out = self.embed(inputs)\n",
        "        lstm_out, *_ = self.lstm(embed_out)\n",
        "        linear_out = self.linear(lstm_out)\n",
        "        return linear_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f2081d-54a6-4867-e766-49fe3bdfd367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 50] Train: Loss = 0.56333, Accuracy = 83.48%: 100%|██████████| 572/572 [00:02<00:00, 200.19it/s]\n",
            "[1 / 50]   Val: Loss = 0.25526, Accuracy = 92.45%: 100%|██████████| 13/13 [00:00<00:00, 93.07it/s]\n",
            "[2 / 50] Train: Loss = 0.18608, Accuracy = 94.50%: 100%|██████████| 572/572 [00:02<00:00, 208.49it/s]\n",
            "[2 / 50]   Val: Loss = 0.17244, Accuracy = 94.74%: 100%|██████████| 13/13 [00:00<00:00, 94.34it/s]\n",
            "[3 / 50] Train: Loss = 0.13240, Accuracy = 96.04%: 100%|██████████| 572/572 [00:02<00:00, 211.36it/s]\n",
            "[3 / 50]   Val: Loss = 0.13962, Accuracy = 95.71%: 100%|██████████| 13/13 [00:00<00:00, 93.51it/s]\n",
            "[4 / 50] Train: Loss = 0.10644, Accuracy = 96.79%: 100%|██████████| 572/572 [00:03<00:00, 174.30it/s]\n",
            "[4 / 50]   Val: Loss = 0.12224, Accuracy = 96.20%: 100%|██████████| 13/13 [00:00<00:00, 78.33it/s]\n",
            "[5 / 50] Train: Loss = 0.09117, Accuracy = 97.23%: 100%|██████████| 572/572 [00:02<00:00, 209.96it/s]\n",
            "[5 / 50]   Val: Loss = 0.11421, Accuracy = 96.41%: 100%|██████████| 13/13 [00:00<00:00, 97.22it/s]\n",
            "[6 / 50] Train: Loss = 0.08117, Accuracy = 97.51%: 100%|██████████| 572/572 [00:02<00:00, 207.45it/s]\n",
            "[6 / 50]   Val: Loss = 0.10553, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 92.51it/s]\n",
            "[7 / 50] Train: Loss = 0.07382, Accuracy = 97.73%: 100%|██████████| 572/572 [00:02<00:00, 212.23it/s]\n",
            "[7 / 50]   Val: Loss = 0.10198, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 95.71it/s]\n",
            "[8 / 50] Train: Loss = 0.06822, Accuracy = 97.89%: 100%|██████████| 572/572 [00:03<00:00, 181.26it/s]\n",
            "[8 / 50]   Val: Loss = 0.09890, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 74.29it/s]\n",
            "[9 / 50] Train: Loss = 0.06351, Accuracy = 98.03%: 100%|██████████| 572/572 [00:02<00:00, 195.19it/s]\n",
            "[9 / 50]   Val: Loss = 0.09776, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 85.13it/s]\n",
            "[10 / 50] Train: Loss = 0.05954, Accuracy = 98.14%: 100%|██████████| 572/572 [00:02<00:00, 203.64it/s]\n",
            "[10 / 50]   Val: Loss = 0.09291, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 87.38it/s]\n",
            "[11 / 50] Train: Loss = 0.05603, Accuracy = 98.24%: 100%|██████████| 572/572 [00:02<00:00, 204.11it/s]\n",
            "[11 / 50]   Val: Loss = 0.09601, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 92.38it/s]\n",
            "[12 / 50] Train: Loss = 0.05296, Accuracy = 98.34%: 100%|██████████| 572/572 [00:03<00:00, 177.53it/s]\n",
            "[12 / 50]   Val: Loss = 0.09189, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 74.52it/s]\n",
            "[13 / 50] Train: Loss = 0.05017, Accuracy = 98.42%: 100%|██████████| 572/572 [00:03<00:00, 188.22it/s]\n",
            "[13 / 50]   Val: Loss = 0.09257, Accuracy = 97.07%: 100%|██████████| 13/13 [00:00<00:00, 90.85it/s]\n",
            "[14 / 50] Train: Loss = 0.04760, Accuracy = 98.49%: 100%|██████████| 572/572 [00:02<00:00, 201.63it/s]\n",
            "[14 / 50]   Val: Loss = 0.09388, Accuracy = 97.02%: 100%|██████████| 13/13 [00:00<00:00, 87.90it/s]\n",
            "[15 / 50] Train: Loss = 0.04534, Accuracy = 98.56%: 100%|██████████| 572/572 [00:02<00:00, 204.58it/s]\n",
            "[15 / 50]   Val: Loss = 0.09383, Accuracy = 97.10%: 100%|██████████| 13/13 [00:00<00:00, 87.69it/s]\n",
            "[16 / 50] Train: Loss = 0.04327, Accuracy = 98.63%: 100%|██████████| 572/572 [00:03<00:00, 182.90it/s]\n",
            "[16 / 50]   Val: Loss = 0.09300, Accuracy = 97.08%: 100%|██████████| 13/13 [00:00<00:00, 73.13it/s]\n",
            "[17 / 50] Train: Loss = 0.04130, Accuracy = 98.71%: 100%|██████████| 572/572 [00:03<00:00, 186.89it/s]\n",
            "[17 / 50]   Val: Loss = 0.09664, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 88.92it/s]\n",
            "[18 / 50] Train: Loss = 0.03951, Accuracy = 98.77%: 100%|██████████| 572/572 [00:02<00:00, 200.24it/s]\n",
            "[18 / 50]   Val: Loss = 0.09493, Accuracy = 97.08%: 100%|██████████| 13/13 [00:00<00:00, 92.36it/s]\n",
            "[19 / 50] Train: Loss = 0.03761, Accuracy = 98.83%: 100%|██████████| 572/572 [00:02<00:00, 200.77it/s]\n",
            "[19 / 50]   Val: Loss = 0.09591, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 85.15it/s]\n",
            "[20 / 50] Train: Loss = 0.03589, Accuracy = 98.87%: 100%|██████████| 572/572 [00:03<00:00, 179.10it/s]\n",
            "[20 / 50]   Val: Loss = 0.09527, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 70.68it/s]\n",
            "[21 / 50] Train: Loss = 0.03438, Accuracy = 98.93%: 100%|██████████| 572/572 [00:03<00:00, 183.38it/s]\n",
            "[21 / 50]   Val: Loss = 0.09920, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 88.71it/s]\n",
            "[22 / 50] Train: Loss = 0.03293, Accuracy = 98.98%: 100%|██████████| 572/572 [00:02<00:00, 196.53it/s]\n",
            "[22 / 50]   Val: Loss = 0.09975, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 89.43it/s]\n",
            "[23 / 50] Train: Loss = 0.03135, Accuracy = 99.03%: 100%|██████████| 572/572 [00:02<00:00, 196.71it/s]\n",
            "[23 / 50]   Val: Loss = 0.10215, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 85.04it/s]\n",
            "[24 / 50] Train: Loss = 0.03020, Accuracy = 99.07%: 100%|██████████| 572/572 [00:03<00:00, 184.39it/s]\n",
            "[24 / 50]   Val: Loss = 0.10116, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 74.98it/s]\n",
            "[25 / 50] Train: Loss = 0.02869, Accuracy = 99.12%: 100%|██████████| 572/572 [00:03<00:00, 184.47it/s]\n",
            "[25 / 50]   Val: Loss = 0.10406, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 88.39it/s]\n",
            "[26 / 50] Train: Loss = 0.02732, Accuracy = 99.17%: 100%|██████████| 572/572 [00:02<00:00, 199.23it/s]\n",
            "[26 / 50]   Val: Loss = 0.10335, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 88.76it/s]\n",
            "[27 / 50] Train: Loss = 0.02626, Accuracy = 99.21%: 100%|██████████| 572/572 [00:02<00:00, 202.63it/s]\n",
            "[27 / 50]   Val: Loss = 0.10699, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 89.98it/s]\n",
            "[28 / 50] Train: Loss = 0.02508, Accuracy = 99.26%: 100%|██████████| 572/572 [00:03<00:00, 186.24it/s]\n",
            "[28 / 50]   Val: Loss = 0.10808, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 75.21it/s]\n",
            "[29 / 50] Train: Loss = 0.02394, Accuracy = 99.29%: 100%|██████████| 572/572 [00:03<00:00, 175.00it/s]\n",
            "[29 / 50]   Val: Loss = 0.10833, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 86.51it/s]\n",
            "[30 / 50] Train: Loss = 0.02290, Accuracy = 99.33%: 100%|██████████| 572/572 [00:02<00:00, 199.97it/s]\n",
            "[30 / 50]   Val: Loss = 0.11226, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 89.74it/s]\n",
            "[31 / 50] Train: Loss = 0.02185, Accuracy = 99.36%: 100%|██████████| 572/572 [00:02<00:00, 201.32it/s]\n",
            "[31 / 50]   Val: Loss = 0.11312, Accuracy = 96.88%: 100%|██████████| 13/13 [00:00<00:00, 91.52it/s]\n",
            "[32 / 50] Train: Loss = 0.02074, Accuracy = 99.41%: 100%|██████████| 572/572 [00:03<00:00, 185.62it/s]\n",
            "[32 / 50]   Val: Loss = 0.11562, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 79.88it/s]\n",
            "[33 / 50] Train: Loss = 0.01977, Accuracy = 99.43%: 100%|██████████| 572/572 [00:03<00:00, 176.57it/s]\n",
            "[33 / 50]   Val: Loss = 0.11496, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 91.84it/s]\n",
            "[34 / 50] Train: Loss = 0.01888, Accuracy = 99.46%: 100%|██████████| 572/572 [00:02<00:00, 199.09it/s]\n",
            "[34 / 50]   Val: Loss = 0.11850, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 86.14it/s]\n",
            "[35 / 50] Train: Loss = 0.01791, Accuracy = 99.49%: 100%|██████████| 572/572 [00:02<00:00, 199.16it/s]\n",
            "[35 / 50]   Val: Loss = 0.12081, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 89.84it/s]\n",
            "[36 / 50] Train: Loss = 0.01714, Accuracy = 99.53%: 100%|██████████| 572/572 [00:03<00:00, 185.17it/s]\n",
            "[36 / 50]   Val: Loss = 0.12297, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 75.13it/s]\n",
            "[37 / 50] Train: Loss = 0.01637, Accuracy = 99.55%: 100%|██████████| 572/572 [00:03<00:00, 174.24it/s]\n",
            "[37 / 50]   Val: Loss = 0.12706, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 87.60it/s]\n",
            "[38 / 50] Train: Loss = 0.01555, Accuracy = 99.58%: 100%|██████████| 572/572 [00:02<00:00, 199.34it/s]\n",
            "[38 / 50]   Val: Loss = 0.12690, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 89.80it/s]\n",
            "[39 / 50] Train: Loss = 0.01466, Accuracy = 99.62%: 100%|██████████| 572/572 [00:02<00:00, 203.30it/s]\n",
            "[39 / 50]   Val: Loss = 0.12972, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 93.96it/s]\n",
            "[40 / 50] Train: Loss = 0.01390, Accuracy = 99.64%: 100%|██████████| 572/572 [00:03<00:00, 189.92it/s]\n",
            "[40 / 50]   Val: Loss = 0.13675, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 79.38it/s]\n",
            "[41 / 50] Train: Loss = 0.01328, Accuracy = 99.65%: 100%|██████████| 572/572 [00:03<00:00, 173.99it/s]\n",
            "[41 / 50]   Val: Loss = 0.13527, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 86.49it/s]\n",
            "[42 / 50] Train: Loss = 0.01238, Accuracy = 99.70%: 100%|██████████| 572/572 [00:02<00:00, 201.14it/s]\n",
            "[42 / 50]   Val: Loss = 0.13819, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 88.67it/s]\n",
            "[43 / 50] Train: Loss = 0.01180, Accuracy = 99.70%: 100%|██████████| 572/572 [00:02<00:00, 197.67it/s]\n",
            "[43 / 50]   Val: Loss = 0.14039, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 88.00it/s]\n",
            "[44 / 50] Train: Loss = 0.01113, Accuracy = 99.73%: 100%|██████████| 572/572 [00:02<00:00, 200.86it/s]\n",
            "[44 / 50]   Val: Loss = 0.14298, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 83.16it/s]\n",
            "[45 / 50] Train: Loss = 0.01048, Accuracy = 99.76%: 100%|██████████| 572/572 [00:03<00:00, 164.09it/s]\n",
            "[45 / 50]   Val: Loss = 0.14847, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 82.28it/s]\n",
            "[46 / 50] Train: Loss = 0.01007, Accuracy = 99.76%: 100%|██████████| 572/572 [00:02<00:00, 198.54it/s]\n",
            "[46 / 50]   Val: Loss = 0.14751, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 87.00it/s]\n",
            "[47 / 50] Train: Loss = 0.00942, Accuracy = 99.78%: 100%|██████████| 572/572 [00:02<00:00, 197.69it/s]\n",
            "[47 / 50]   Val: Loss = 0.15619, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 92.42it/s]\n",
            "[48 / 50] Train: Loss = 0.00901, Accuracy = 99.80%: 100%|██████████| 572/572 [00:02<00:00, 197.55it/s]\n",
            "[48 / 50]   Val: Loss = 0.15511, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 92.68it/s]\n",
            "[49 / 50] Train: Loss = 0.00839, Accuracy = 99.81%: 100%|██████████| 572/572 [00:03<00:00, 167.75it/s]\n",
            "[49 / 50]   Val: Loss = 0.15581, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 70.72it/s]\n",
            "[50 / 50] Train: Loss = 0.00793, Accuracy = 99.83%: 100%|██████████| 572/572 [00:03<00:00, 189.10it/s]\n",
            "[50 / 50]   Val: Loss = 0.16007, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 86.78it/s]\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a85f57f-dce5-41e3-c70f-c75f2dda0b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: Loss = 0.15969, Accuracy = 96.70%: 100%|██████████| 224/224 [00:00<00:00, 303.19it/s]\n"
          ]
        }
      ],
      "source": [
        "batch_size=64\n",
        "test_data=(X_test, y_test)\n",
        "test_loss, test_acc = do_epoch(model, criterion, test_data, batch_size, None, 'Train:')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}